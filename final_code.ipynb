{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Thesis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Author: Mathieu Leng\n",
    "Date: December 2023\n",
    "Version: 1\n",
    "Goal: This code is related with a master thesis written by Mathieu Leng, under the supervision of Pr. Miguel Nogueira, for Catolica Lisbon School of Business and Economics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # to handle dataset                                   \n",
    "import xarray as xr  # to handle dataset from CORDEX and ERA5 sources\n",
    "import numpy as np   # to handle dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt   # to plot\n",
    "import re   # to use regular expression\n",
    "from UTCI.Compute_UTCI import Compute_Tmrt, Compute_UTCI_approx   # code from Pr. Nogeira, to calculate UTCI\n",
    "import gurobipy as gp  # to do linear programming\n",
    "from gurobipy import GRB  # to do linear programming\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns  # to plot\n",
    "from matplotlib.patches import Patch\n",
    "import unidecode\n",
    "from scipy.stats import chi2_contingency\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from xclim import sdba\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1 Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def custom_round(x):\n",
    "    \"\"\"\n",
    "    Round the value to the quarter\n",
    "    :param x: a number\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    return np.ceil(x * 4) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_col(df, col_name):\n",
    "    \"\"\"\n",
    "    Cleans the column\n",
    "    :param df: a pandas dataframe\n",
    "    :param col_name: name of the columns to clean\n",
    "    :return: \n",
    "        the dataframe\n",
    "    \"\"\"\n",
    "    df[f'{col_name}'] = df[f'{col_name}'].str.replace(',', '.').astype(float)\n",
    "    df[f'{col_name}'] = df[f'{col_name}'].apply(custom_round)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to remove accents from a string\n",
    "def remove_accents(input_str):\n",
    "    return unidecode.unidecode(input_str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 file handling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_date(f):\n",
    "    \"\"\"\n",
    "    Creates a date object, getting it from the name of a file\n",
    "    :param f: file name\n",
    "    :return: date_obj: date object\n",
    "    \"\"\"\n",
    "    pattern = r'_(\\d{8})_'  # regular expression\n",
    "    match = re.search(pattern, f) # find matching expressions\n",
    "    date = match.group(1) # get the second element of the matching expression\n",
    "    date_obj = pd.to_datetime(date, format='%Y%m%d') \n",
    "    return date_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extend_years(df, begin, end):\n",
    "    \"\"\"\n",
    "    extends the Date from begin to end year\n",
    "    :param df: a pandas dataframe\n",
    "    :param begin: begin year \n",
    "    :param end: end year\n",
    "    :return: \n",
    "        dataframe with extended years\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    all_years_df = pd.DataFrame()\n",
    "    \n",
    "    for year in range(begin, end): # Loop over desired years\n",
    "        tempo_df = df.copy()\n",
    "        tempo_df['Date'] = tempo_df['Date'].apply(lambda x: x.replace(year=year))\n",
    "        all_years_df = pd.concat([all_years_df, tempo_df])\n",
    "\n",
    "    all_years_df.reset_index(drop=True, inplace=True)\n",
    "    return all_years_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_fake_features(df):\n",
    "    \"\"\"\n",
    "    creates fake games for multiple hours \n",
    "    \"\"\"\n",
    "\n",
    "    new_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        for hour in [12, 15, 18, 21]:\n",
    "            if row['Hour'] != hour:\n",
    "                new_row = row.copy()\n",
    "                new_row['Match Time(CET)'] = pd.Timestamp(row['Date']).replace(hour=hour, minute=0, second=0)\n",
    "                new_row['Hour'] = hour\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "    \n",
    "    new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    df['True_Game'] = 1\n",
    "    new_rows_df['True_Game'] = 0\n",
    "\n",
    "    combined_df = pd.concat([df, new_rows_df], ignore_index=True)\n",
    "\n",
    "    combined_df.sort_values(by='Match Time(CET)', inplace=True)\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_timeseries_at_lat_lon_time(da: xr.DataArray, target_lat: float, target_lon: float, timestamp: str) -> float:\n",
    "    \"\"\"\n",
    "    Gets the utci value according to the latitude, longitude, and timestamp\n",
    "    :param da: Data Array\n",
    "    :param target_lat: value of the target latitude \n",
    "    :param target_lon: value of the target latitude \n",
    "    :param timestamp: timestamp\n",
    "    :return: selected_data: \n",
    "    \"\"\"\n",
    "    variable_da = da['utci']\n",
    "    timestamp = pd.to_datetime(timestamp) # Convert timestamp to pandas datetime to match the format in da\n",
    "\n",
    "    # Get the nearest latitude and longitude\n",
    "    nearest_lat = variable_da.sel(lat=target_lat, method='nearest').lat.values\n",
    "    nearest_lon = variable_da.sel(lon=target_lon, method='nearest').lon.values\n",
    "\n",
    "    # Select data based on nearest latitude, longitude, and exact timestamp\n",
    "    selected_data = variable_da.sel(lat=nearest_lat, lon=nearest_lon,time=timestamp,method='nearest').values\n",
    "    \n",
    "    return selected_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_timeseries_at_rlat_rlon_time(da: xr.DataArray,target_lat: float,target_lon: float, timestamp: np.datetime64) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    return the value from da, having corresponding timestamp, latitude, and longitude\n",
    "    :param da: the to-be merged dataset\n",
    "    :param target_lat: the target latitude\n",
    "    :param target_lon: the target longitude\n",
    "    :param timestamp: timestamp\n",
    "    :return: \n",
    "        res: corresponding value\n",
    "    \"\"\"\n",
    "    sliced_ds=da.sel(time=timestamp,method='nearest') # slice for only the correct date\n",
    "    i,j=np.unravel_index(np.argmin(np.sqrt( (sliced_ds['lat'].values-target_lat)**2+ (sliced_ds['lon'].values-target_lon)**2)),da['lat'].shape) # calculates row and col  index to the closest data from target lat and long\n",
    "    res = float(sliced_ds.values[i,j])\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_var_for_row(row, da):\n",
    "    \"\"\"\n",
    "    calls another function, sending the right parameters\n",
    "    :param row: line in the original dataset\n",
    "    :param da: the to-be merged dataset\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    return get_timeseries_at_rlat_rlon_time(da, row['latitude'], row['longitude'], row['timestamp'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_1_year(folders_path, result):\n",
    "    \"\"\"\n",
    "    fill one year of data\n",
    "    :param folders_path: path to the folder\n",
    "    :param result: final dataframe will all the data\n",
    "    :return: \n",
    "        result: final dataframe will all the data\n",
    "    \"\"\"\n",
    "    nc_files = [f for f in os.listdir(folders_path) if f.endswith('.nc')]\n",
    "    for file in nc_files: #  each file is one year\n",
    "        with xr.open_dataset(os.path.join(folders_path, file)) as ds:\n",
    "            variable = list(ds.variables)[-1]  # each file is adifferent varibale\n",
    "            year = list(ds['time'].dt.year.values)[0] # get the year of this file\n",
    "            result.loc[result[\"year\"] == year,f'{variable}'] = result[result[\"year\"] == year].apply(get_var_for_row, da=ds[f'{variable}'], axis=1)\n",
    "    return result\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3 Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_UTCI(df):\n",
    "    \"\"\"\n",
    "    Calculates utci value\n",
    "    :param df: pandas dataframe\n",
    "    :return: \n",
    "        the pandas dataframe with UTCI values\n",
    "    \"\"\"\n",
    "    df['tas']=df['tas']-273.15\n",
    "    df['rlus']=df['rsds']-df['rsus']+df['rlds']\n",
    "    Tmrt=Compute_Tmrt(df['rlus'],df['rlds'],df['rsus'],df['rsds'])\n",
    "    Tmrt=Tmrt-273.15\n",
    "    UTCI=Compute_UTCI_approx(df['tas'],df['hurs'],Tmrt,df['sfcWind'])\n",
    "    df[\"UTCI\"] = UTCI\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def categorize_stress(df):\n",
    "    \"\"\"\n",
    "    Add a new column Stress Category based on the UTCI heat stress categories\n",
    "    :param df: pandas dataframe\n",
    "    :return: \n",
    "        Dataframe with new column Stress Category\n",
    "    \"\"\"\n",
    "    conditions = [\n",
    "        (df['utci'] > 46),\n",
    "        (df['utci'] >= 38) & (df['utci'] <= 46),\n",
    "        (df['utci'] >= 32) & (df['utci'] < 38),\n",
    "        (df['utci'] >= 26) & (df['utci'] < 32),\n",
    "        (df['utci'] >= 9) & (df['utci'] < 26),\n",
    "        (df['utci'] >= 0) & (df['utci'] < 9),\n",
    "        (df['utci'] >= -13) & (df['utci'] < 0),\n",
    "        (df['utci'] >= -27) & (df['utci'] < -13),\n",
    "        (df['utci'] >= -40) & (df['utci'] < -27),\n",
    "        (df['utci'] < -40)\n",
    "    ]\n",
    "\n",
    "    choices = [\n",
    "        'Extreme heat stress',\n",
    "        'Very strong heat stress',\n",
    "        'Strong heat stress',\n",
    "        'Moderate heat stress',\n",
    "        'No thermal stress',\n",
    "        'Slight cold stress',\n",
    "        'Moderate cold stress',\n",
    "        'Strong cold stress',\n",
    "        'Very strong cold stress',\n",
    "        'Extreme cold stress'\n",
    "    ]\n",
    "\n",
    "    df['Stress Category'] = np.select(conditions, choices, default=np.nan)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assign_authorities_conditions(utci):\n",
    "    \"\"\"\n",
    "    Matches policies and UTCI values\n",
    "    :param utci: UTCI value\n",
    "    :return: \n",
    "        the policy matching the UTCI value\n",
    "    \"\"\"\n",
    "    if utci >= 46: # approximation bc  WBGT 32 not in the table\n",
    "        return \"FIFA cooling break mandatory\"\n",
    "    elif utci >= 38: # 28 WBGT is 38 UTCI, limitation because not exactly like that in the table \n",
    "        return \"FIFPro game rescheduled\"\n",
    "    elif utci >= 35.8: # ± 26 WBGT\n",
    "        return \"FIFPro cooling breaks mandatory\"\n",
    "    else:\n",
    "        return \"No policies\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.4 Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def silhouette_score_clusters(df, range_k):\n",
    "    \"\"\"\n",
    "    Calculate the silhouette score \n",
    "    :param df: pandas dataframe\n",
    "    :param range_k: range of k \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Grouping the data by location and calculating the median and standard deviation of UTCI values\n",
    "    location_utci_stats = df.groupby('Home Team')['utci'].agg(['median', 'std']).reset_index()\n",
    "\n",
    "    # Normalizing the median and standard deviation of UTCI values\n",
    "    scaler = StandardScaler()\n",
    "    location_utci_stats[['median_normalized', 'std_normalized']] = scaler.fit_transform(\n",
    "        location_utci_stats[['median', 'std']])\n",
    "    \n",
    "    print(location_utci_stats[['median_normalized', 'std_normalized']].isna().any())\n",
    "    # Handling NaN values\n",
    "    location_utci_stats[['median_normalized', 'std_normalized']] = location_utci_stats[\n",
    "        ['median_normalized', 'std_normalized']].fillna(0)\n",
    "\n",
    "    # Finding the optimal number of clusters using silhouette scores\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for k in range_k:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        cluster_labels = kmeans.fit_predict(location_utci_stats[['median_normalized', 'std_normalized']])\n",
    "        silhouette_avg = silhouette_score(location_utci_stats[['median_normalized', 'std_normalized']], cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range_k, silhouette_scores, marker='o')\n",
    "    plt.title('Silhouette Scores for Different Numbers of Clusters')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.xticks(range_k)\n",
    "    plt.grid(False)  # Disable the grid\n",
    "    plt.gca().set_facecolor('white')  # Set the background color to white\n",
    "\n",
    "    # Save the plot\n",
    "    silhouette_file_path = 'plots/silhouette_plot.png'\n",
    "    plt.savefig(silhouette_file_path, bbox_inches='tight')  # Save with tight bounding box\n",
    "\n",
    "    # Display the silhouette scores\n",
    "    return silhouette_scores, location_utci_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize_clusters(df):\n",
    "    \"\"\"\n",
    "    Calculate the optimized clusters\n",
    "    :param df: pandas dataframe\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    range_k = range(2, 11)  # Testing for 2 to 10 clusters\n",
    "    silhouette_scores, location_utci_stats = silhouette_score_clusters(df, range_k)\n",
    "\n",
    "    # Choosing the best K based on the highest silhouette score\n",
    "    best_k = range_k[np.argmax(silhouette_scores)]\n",
    "\n",
    "    # Clustering with the optimal number of clusters\n",
    "    kmeans_optimal = KMeans(n_clusters=best_k, random_state=0)\n",
    "    location_utci_stats['Optimal_Cluster'] = kmeans_optimal.fit_predict(location_utci_stats[['median_normalized', 'std_normalized']])\n",
    "\n",
    "    # Plotting the results with the optimal number of clusters\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.scatterplot(x=location_utci_stats['Home Team'], y=location_utci_stats['median'],\n",
    "                    size=location_utci_stats['std'], hue=location_utci_stats['Optimal_Cluster'], palette='Set1')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(f'K-Means Clustering of Locations Based on UTCI Median and Standard Deviation ({best_k} Clusters)')\n",
    "    plt.xlabel('Home Team')\n",
    "    plt.ylabel('Median UTCI Value')\n",
    "    plt.legend(title='Optimal Cluster')\n",
    "\n",
    "    plt.show()\n",
    "    return location_utci_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_locations_histogram(df):\n",
    "    \"\"\"\n",
    "    Plots the number of games per location \n",
    "    :param df: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Count the frequency of each category\n",
    "    category_counts = df['Home Team'].value_counts()\n",
    "\n",
    "    # Plot the histogram\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    category_counts.plot(kind='bar')\n",
    "    plt.title('Histogram of Number of Features by Teams')\n",
    "    plt.xlabel('Locations')\n",
    "    plt.ylabel('Number of Games')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate labels for better readability\n",
    "    plt.tight_layout()  # Adjust layout for better fit\n",
    "    \n",
    "    # Remove the grid and set the background color to white\n",
    "    plt.grid(False)\n",
    "    plt.gca().set_facecolor('white')\n",
    "    \n",
    "    plt.savefig(\"plots/games_by_locations.png\", )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_overall_percentage_stress_category(df, title, stress_category = 'Stress Category'):\n",
    "    \"\"\"\n",
    "    Plots the percentage per heat stress category\n",
    "    :param df: \n",
    "    :param title: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Calculate the percentage of each stress category\n",
    "    stress_category_percentages = df[stress_category].value_counts(normalize=True) * 100\n",
    "\n",
    "    # Color mapping for the stress categories\n",
    "    colors = {'No thermal stress': 'green', 'Moderate heat stress': 'orange',\n",
    "              'Strong heat stress': 'red', 'Very strong heat stress': 'darkred',\n",
    "              'Extreme heat stress': 'black', 'Slight cold stress': 'lightblue'}\n",
    "\n",
    "    # Ordering the stress categories for consistent color mapping\n",
    "    ordered_categories = stress_category_percentages.index.map(colors)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    stress_category_percentages.plot(kind='bar', ax=ax, color=ordered_categories)\n",
    "    fig.patch.set_facecolor('white')  # Set the background color of the figure to white\n",
    "    ax.set_facecolor('white')  # Set the background color of the axes to white\n",
    "    \n",
    "\n",
    "    # Removing grid\n",
    "    ax.grid(False)\n",
    "\n",
    "    # Adding percentage annotations on each bar\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f\"{p.get_height():.1f}%\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_xlabel('Stress Category')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots_story/{title}.png')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_overall_percentage_policies_category(df, title, policies = 'policies'):\n",
    "    \"\"\"\n",
    "    Plots the percentage per policy category\n",
    "    :param df: \n",
    "    :param title: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Calculate the percentage of each stress category\n",
    "    stress_category_percentages = df[policies].value_counts(normalize=True) * 100\n",
    "\n",
    "    # Color mapping for the stress categories\n",
    "    colors = {\"No policies\": 'green', \"FIFPro cooling breaks mandatory\": 'orange',\n",
    "              \"FIFPro game rescheduled\": 'red', \"FIFA cooling break mandatory\": 'darkred'}\n",
    "\n",
    "    # Ordering the stress categories for consistent color mapping\n",
    "    ordered_categories = stress_category_percentages.index.map(colors)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    stress_category_percentages.plot(kind='bar', ax=ax, color=ordered_categories)\n",
    "    fig.patch.set_facecolor('white')  # Set the background color of the figure to white\n",
    "    ax.set_facecolor('white')  # Set the background color of the axes to white\n",
    "\n",
    "\n",
    "    # Removing grid\n",
    "    ax.grid(False)\n",
    "\n",
    "    # Adding percentage annotations on each bar\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f\"{p.get_height():.1f}%\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_xlabel('Policies Category')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots_story/{title}.png')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_optimal_cluster_percentage_stress_category_histogram(df, title):\n",
    "    grouped_data = df.groupby(['Optimal_Cluster', 'Stress Category']).size().unstack().fillna(0)\n",
    "\n",
    "    # Convert counts to percentages\n",
    "    grouped_percentage = grouped_data.div(grouped_data.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Define the desired order of stress categories\n",
    "    order = ['No thermal stress', 'Moderate heat stress', 'Strong heat stress',\n",
    "             'Very strong heat stress', 'Slight cold stress']\n",
    "\n",
    "    # Reorder the columns of grouped_percentage according to the defined order\n",
    "    grouped_percentage = grouped_percentage[order]\n",
    "\n",
    "    # Color mapping for the stress categories\n",
    "    colors = {'No thermal stress': 'green', 'Moderate heat stress': 'orange',\n",
    "              'Strong heat stress': 'red', 'Very strong heat stress': 'darkred',\n",
    "              'Extreme heat stress': 'black', 'Slight cold stress': 'lightblue',\n",
    "              'Moderate cold stress': 'blue'}\n",
    "\n",
    "    # Ordering the stress categories for consistent color mapping\n",
    "    ordered_categories = [colors[category] for category in order]\n",
    "\n",
    "    # Plot the histogram\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    fig.patch.set_facecolor('white')  # Set the background color of the figure to white\n",
    "    ax.set_facecolor('white')  # Set the background color of the axes to white\n",
    "    grouped_percentage.plot(kind='bar', stacked=True, color=ordered_categories, ax=ax)\n",
    "\n",
    "    # Adding percentage annotations on each segment of the stacked bars\n",
    "    for bars in ax.containers:\n",
    "        ax.bar_label(bars, fmt='%.1f%%', label_type='center')\n",
    "\n",
    "    # Remove the grid\n",
    "    ax.grid(False)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Optimal Cluster')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate labels for better readability\n",
    "    plt.legend(title='Stress Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()  # Adjust layout for better fit\n",
    "    # Remove the spines (the rectangle around the plot)\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_visible(False)\n",
    "    # Save the plot to a file\n",
    "    plt.savefig(f'plots_story/{title}.png', facecolor=fig.get_facecolor(), edgecolor='none')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_grouped_percentage_stress_category_histogram(df):\n",
    "    # Group the data by 'Hour' and then by 'Stress Category'\n",
    "    grouped_data = df.groupby(['Hour', 'Stress Category']).size().unstack().fillna(0)\n",
    "\n",
    "    # Convert counts to percentages\n",
    "    grouped_percentage = grouped_data.div(grouped_data.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Define the desired order of stress categories\n",
    "    order = ['No thermal stress', 'Moderate heat stress', 'Strong heat stress',\n",
    "             'Very strong heat stress', 'Extreme heat stress', 'Slight cold stress']\n",
    "\n",
    "    # Reorder the columns of grouped_percentage according to the defined order\n",
    "    grouped_percentage = grouped_percentage[order]\n",
    "\n",
    "    # Color mapping for the stress categories\n",
    "    colors = {'No thermal stress': 'green', 'Moderate heat stress': 'orange',\n",
    "              'Strong heat stress': 'red', 'Very strong heat stress': 'darkred',\n",
    "              'Extreme heat stress': 'black', 'Slight cold stress': 'lightblue',\n",
    "              'Moderate cold stress': 'blue'}\n",
    "\n",
    "    # Ordering the stress categories for consistent color mapping\n",
    "    ordered_categories = [colors[category] for category in order]\n",
    "\n",
    "    # Plot the histogram\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    grouped_percentage.plot(kind='bar', stacked=True, color=ordered_categories, ax=ax)\n",
    "\n",
    "    # Set background color to white\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    # Remove the grid\n",
    "    ax.grid(False)\n",
    "\n",
    "    # Adding percentage annotations on each segment of the stacked bars\n",
    "    for bars in ax.containers:\n",
    "        ax.bar_label(bars, fmt='%.1f%%', label_type='center')\n",
    "\n",
    "    # Set titles and labels\n",
    "    plt.title('Percentage Histogram of Stress Categories Grouped by Hour')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Stress Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Remove the spines (the rectangle around the plot)\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig('plots_story/grouped_stress_category_histogram_by_hour.png', facecolor=fig.get_facecolor(), edgecolor='none')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_grouped_month_percentage_stress_category_histogram(df):\n",
    "    # Group the data by 'Month' and then by 'Stress Category'\n",
    "    grouped_data = df.groupby(['month', 'Stress Category']).size().unstack().fillna(0)\n",
    "\n",
    "    # Convert counts to percentages\n",
    "    grouped_percentage = grouped_data.div(grouped_data.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Define the desired order of stress categories\n",
    "    order = ['No thermal stress', 'Moderate heat stress', 'Strong heat stress',\n",
    "             'Very strong heat stress', 'Extreme heat stress', 'Slight cold stress']\n",
    "\n",
    "    # Reorder the columns of grouped_percentage according to the defined order\n",
    "    grouped_percentage = grouped_percentage[order]\n",
    "\n",
    "    # Color mapping for the stress categories\n",
    "    colors = {'No thermal stress': 'green', 'Moderate heat stress': 'orange',\n",
    "              'Strong heat stress': 'red', 'Very strong heat stress': 'darkred',\n",
    "              'Extreme heat stress': 'black', 'Slight cold stress': 'lightblue',\n",
    "              'Moderate cold stress': 'blue'}\n",
    "\n",
    "    # Ordering the stress categories for consistent color mapping\n",
    "    ordered_categories = [colors[category] for category in order]\n",
    "\n",
    "    # Plot the histogram\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    grouped_percentage.plot(kind='bar', stacked=True, color=ordered_categories, ax=ax)\n",
    "\n",
    "    # Set background color to white, remove grid and spines\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax.set_facecolor('white')\n",
    "    ax.grid(False)\n",
    "\n",
    "    # Remove the spines (the rectangle around the plot)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    # Adding percentage annotations on each segment of the stacked bars\n",
    "    for bars in ax.containers:\n",
    "        ax.bar_label(bars, fmt='%.1f%%', label_type='center')\n",
    "\n",
    "    # Set titles and labels\n",
    "    plt.title('Percentage Histogram of Stress Categories Grouped by Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Stress Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig('plots_story/grouped_stress_category_histogram_by_month.png', facecolor=fig.get_facecolor(), edgecolor='none')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_stress_category_percentage(df):\n",
    "    \"\"\"\n",
    "    Plot the percentage of stress categories over years for each home team in the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input dataframe containing the stress category data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Group the original data by 'Home Team', 'year', and 'Stress Category'\n",
    "    category_counts_per_team_year = df.groupby(['Home Team', 'year', 'Stress Category']).size().reset_index(name='count')\n",
    "    total_counts_per_team_year = df.groupby(['Home Team', 'year']).size().reset_index(name='total_count')\n",
    "\n",
    "    # Merge and calculate percentages\n",
    "    category_percentage_per_team = category_counts_per_team_year.merge(total_counts_per_team_year, on=['Home Team', 'year'])\n",
    "    category_percentage_per_team['percentage'] = (category_percentage_per_team['count'] / category_percentage_per_team['total_count']) * 100\n",
    "\n",
    "    # Get a list of unique Home Teams\n",
    "    home_teams = category_percentage_per_team['Home Team'].unique()\n",
    "\n",
    "    # Plot settings\n",
    "    colors = {\n",
    "        'Very strong heat stress': 'darkred',\n",
    "        'Strong heat stress': 'red',\n",
    "        'No thermal stress': 'darkgreen'  # Setting the color for 'No thermal stress'\n",
    "    }\n",
    "\n",
    "    # Plot the line plots for each Home Team\n",
    "    for team in home_teams:\n",
    "        team_data = category_percentage_per_team[category_percentage_per_team['Home Team'] == team]\n",
    "        pivot_data = team_data.pivot(index='year', columns='Stress Category', values='percentage')\n",
    "        pivot_data = pivot_data.fillna(0)  # Fill NaN values with 0\n",
    "\n",
    "        # Plotting\n",
    "        ax = pivot_data.plot(kind='line', marker='o', title=f'Stress Category Percentage Over Years for {team}',\n",
    "                             color=[colors.get(x, '#1f77b4') for x in pivot_data.columns])\n",
    "        ax.set_ylabel('Percentage')\n",
    "        ax.set_xlabel('Year')\n",
    "        ax.grid(True)\n",
    "        ax.legend(title='Stress Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_stress_category_distribution_by_hour(dataframe, utci_column, stress_column, title):\n",
    "    \"\"\"\n",
    "    Plot a bar chart with custom colors and values showing the distribution of stress categories for a given UTCI category across hours.\n",
    "\n",
    "    :param dataframe: DataFrame containing the data.\n",
    "    :param utci_column: Column name for the UTCI category.\n",
    "    :param stress_column: Column name for the stress category.\n",
    "    :param title: Title for the plot.\n",
    "    \"\"\"\n",
    "    # Custom colors for stress categories\n",
    "    colors = {'No thermal stress': 'green', 'Moderate heat stress': 'orange',\n",
    "              'Strong heat stress': 'red', 'Very strong heat stress': 'darkred',\n",
    "              'Extreme heat stress': 'black', 'Slight cold stress': 'lightblue'}\n",
    "\n",
    "    # Filter the dataset for the specific UTCI category and group by hour and stress category\n",
    "    hour_stress_group = dataframe.groupby(['hour', stress_column]).size().unstack(fill_value=0)\n",
    "\n",
    "    # Reorder the columns to include all possible categories\n",
    "    all_categories = ['No thermal stress', 'Moderate heat stress', 'Strong heat stress',\n",
    "                      'Very strong heat stress', 'Extreme heat stress', 'Slight cold stress']\n",
    "    hour_stress_group = hour_stress_group.reindex(columns=all_categories, fill_value=0)\n",
    "\n",
    "    # Calculate percentages instead of counts\n",
    "    hour_stress_percent = hour_stress_group.div(hour_stress_group.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Plotting\n",
    "    ax = hour_stress_percent.plot(kind='bar', stacked=True, figsize=(12, 8),\n",
    "                                  color=[colors.get(x, '#333333') for x in all_categories])\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.legend(title='Stress Category')\n",
    "    ax.set_facecolor('white')  # Set the background color to white\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0f}%'.format(y))) # Format y-axis labels as percentages\n",
    "\n",
    "    # Adding values on top of each bar\n",
    "    for p in ax.patches:\n",
    "        width = p.get_width()\n",
    "        height = p.get_height()\n",
    "        x, y = p.get_xy()\n",
    "        if height > 0: # To avoid displaying 0%\n",
    "            ax.annotate(f'{height:.0f}%', (x + width/2, y + height/2), ha='center')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    plt.savefig(f\"plots/{title}.pdf\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_stress_category_distribution_with_values(dataframe, stress_column, title):\n",
    "    \"\"\"\n",
    "    Plot a bar chart with custom colors and values showing the distribution of stress categories for a given UTCI category across months.\n",
    "\n",
    "    :param dataframe: DataFrame containing the data.\n",
    "    :param stress_column: Column name for the stress category.\n",
    "    :param title: Title for the plot.\n",
    "    \"\"\"\n",
    "    # Custom colors for stress categories\n",
    "    colors = {'No thermal stress': 'green', 'Moderate heat stress': 'orange',\n",
    "              'Strong heat stress': 'red', 'Very strong heat stress': 'darkred',\n",
    "              'Extreme heat stress': 'black', 'Slight cold stress': 'lightblue'}\n",
    "\n",
    "    # Filter the dataset for the specific UTCI category and group by month and stress category\n",
    "    month_stress_group = dataframe.groupby(['month', stress_column]).size().unstack(fill_value=0)\n",
    "\n",
    "    # Reorder the columns to include all possible categories\n",
    "    all_categories = ['No thermal stress', 'Moderate heat stress', 'Strong heat stress',\n",
    "                      'Very strong heat stress', 'Extreme heat stress', 'Slight cold stress']\n",
    "    month_stress_group = month_stress_group.reindex(columns=all_categories, fill_value=0)\n",
    "\n",
    "    # Calculate percentages instead of counts\n",
    "    month_stress_percent = month_stress_group.div(month_stress_group.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Plotting\n",
    "    ax = month_stress_percent.plot(kind='bar', stacked=True, figsize=(12, 8),\n",
    "                                   color=[colors.get(x, '#333333') for x in all_categories])\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.legend(title='Stress Category')\n",
    "    ax.set_facecolor('white')  # Set the background color to white\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0f}%'.format(y))) # Format y-axis labels as percentages\n",
    "\n",
    "    # Adding values on top of each bar\n",
    "    for p in ax.patches:\n",
    "        width = p.get_width()\n",
    "        height = p.get_height()\n",
    "        x, y = p.get_xy()\n",
    "        if height > 0: # To avoid displaying 0%\n",
    "            ax.annotate(f'{height:.0f}%', (x + width/2, y + height/2), ha='center')\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    \n",
    "    plt.savefig(f\"plots/{title}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def categorize_stress_future(df, col, stress_cat):\n",
    "    conditions = [\n",
    "        (df[f'{col}'] > 46),\n",
    "        (df[f'{col}'] >= 38) & (df[f'{col}'] <= 46),\n",
    "        (df[f'{col}'] >= 32) & (df[f'{col}'] < 38),\n",
    "        (df[f'{col}'] >= 26) & (df[f'{col}'] < 32),\n",
    "        (df[f'{col}'] >= 9) & (df[f'{col}'] < 26),\n",
    "        (df[f'{col}'] >= 0) & (df[f'{col}'] < 9),\n",
    "        (df[f'{col}'] >= -13) & (df[f'{col}'] < 0),\n",
    "        (df[f'{col}'] >= -27) & (df[f'{col}'] < -13),\n",
    "        (df[f'{col}'] >= -40) & (df[f'{col}'] < -27),\n",
    "        (df[f'{col}'] < -40)\n",
    "    ]\n",
    "\n",
    "    choices = [\n",
    "        'Extreme heat stress',\n",
    "        'Very strong heat stress',\n",
    "        'Strong heat stress',\n",
    "        'Moderate heat stress',\n",
    "        'No thermal stress',\n",
    "        'Slight cold stress',\n",
    "        'Moderate cold stress',\n",
    "        'Strong cold stress',\n",
    "        'Very strong cold stress',\n",
    "        'Extreme cold stress'\n",
    "    ]\n",
    "\n",
    "    df[f'{stress_cat}'] = np.select(conditions, choices, default=np.nan)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Updated function to plot stress categories on the x-axis with specified color legend for each stress type\n",
    "def plot_stress_category_distribution_colored(df, stress_columns):\n",
    "    \"\"\"\n",
    "    Plots a bar chart of the percentage of each stress type within each stress category with specified colors.\n",
    "\n",
    "    :param df: DataFrame containing the data.\n",
    "    :param stress_columns: List of column names containing stress category data.\n",
    "    \"\"\"\n",
    "    # Define the color mapping for each stress type\n",
    "    color_mapping = {\n",
    "        'No thermal stress': 'green',\n",
    "        'Slight cold stress': 'aqua',\n",
    "        'Moderate heat stress': 'darkorange',\n",
    "        'Moderate cold stress': 'deepskyblue',\n",
    "        'Strong heat stress': 'red',\n",
    "        'Very strong heat stress': 'maroon',\n",
    "        'Extreme heat stress': 'purple'\n",
    "    }\n",
    "\n",
    "    # Initialize an empty DataFrame to store percentage values\n",
    "    stress_percentages = pd.DataFrame()\n",
    "\n",
    "    # Calculate the percentage of each stress type for each stress category\n",
    "    for column in stress_columns:\n",
    "        stress_counts = df[column].value_counts(normalize=True) * 100\n",
    "        stress_percentages[column] = stress_counts\n",
    "\n",
    "    # Transpose the DataFrame for plotting\n",
    "    stress_percentages = stress_percentages.T.fillna(0)\n",
    "\n",
    "    # Prepare a list of colors based on the stress types\n",
    "    colors = [color_mapping.get(x, 'grey') for x in stress_percentages.columns]\n",
    "\n",
    "    # Plot\n",
    "    ax = stress_percentages.plot(kind='bar', stacked=True, color=colors, figsize=(14, 7))\n",
    "\n",
    "    # Annotate the percentages on the bars\n",
    "    for c in ax.containers:\n",
    "        labels = [f'{v.get_height():.1f}%' if v.get_height() > 0 else '' for v in c]\n",
    "        ax.bar_label(c, labels=labels, label_type='center')\n",
    "\n",
    "    legend_patches = [Patch(color=color, label=label) for label, color in color_mapping.items()]\n",
    "    plt.legend(handles=legend_patches, title='Stress Types')\n",
    "\n",
    "    plt.title('Percentage of each Stress Type by Stress Category')\n",
    "    plt.xlabel('Stress Category')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()  # Adjust the plot to ensure everything fits without overlapping\n",
    "\n",
    "    # Remove the grid and set the background color to white\n",
    "    plt.grid(False)\n",
    "    plt.gca().set_facecolor('white')\n",
    "    # Remove the spines (the rectangle around the plot)\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_visible(False)\n",
    "    plt.savefig(\"plots/percentage_different_ssp.pdf\", format='pdf', bbox_inches='tight')    \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_and_save_stress_category_percentage_ordered(df, rcp_column, rcp_value, save_path, color_mapping, order):\n",
    "    \"\"\"\n",
    "    Plots and saves the stress category percentage bar chart for a given RCP with specified colors and order.\n",
    "\n",
    "    :param df: DataFrame containing the dataset.\n",
    "    :param rcp_column: The column name representing the stress category for the specific RCP.\n",
    "    :param rcp_value: The RCP value (e.g., '2.6', '4.5', '8.5') used in the plot title.\n",
    "    :param save_path: Path where the plot image will be saved.\n",
    "    :param color_mapping: Dictionary mapping stress categories to colors.\n",
    "    :param order: The order in which to display the stress categories.\n",
    "    \"\"\"\n",
    "    # Group by Optimal_Cluster and Stress Category, then count\n",
    "    counts = df.groupby(['Optimal_Cluster', rcp_column]).size().unstack(fill_value=0)\n",
    "\n",
    "    # Reorder the columns according to the specified order\n",
    "    counts = counts[order]\n",
    "\n",
    "    # Calculate percentages\n",
    "    percentages = counts.div(counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Colors for each category in the specified order\n",
    "    category_colors = [color_mapping.get(x, 'gray') for x in order]\n",
    "\n",
    "    # Plotting\n",
    "    ax = percentages.plot(kind='bar', stacked=True, color=category_colors, figsize=(10, 6), legend=True)\n",
    "    plt.title(f'Stress Category Percentages for RCP {rcp_value}')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xlabel('Optimal_Cluster')\n",
    "    plt.grid(False)\n",
    "    ax.set_facecolor('white')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "    # Annotating percentages on the bars\n",
    "    for n, x in enumerate([*percentages.index.values]):\n",
    "        for (proportion, y_loc) in zip(percentages.loc[x],\n",
    "                                       percentages.loc[x].cumsum()):\n",
    "            plt.text(x=n, y=(y_loc - proportion) + (proportion / 2),\n",
    "                     s=f'{proportion:.1f}%', ha='center', va='center', fontsize=8)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "color_mapping = {\n",
    "    'No thermal stress': 'green',\n",
    "    'Slight cold stress': 'aqua',\n",
    "    'Moderate heat stress': 'darkorange',\n",
    "    'Moderate cold stress': 'deepskyblue',\n",
    "    'Strong heat stress': 'red',\n",
    "    'Very strong heat stress': 'maroon',\n",
    "    'Extreme heat stress': 'purple'\n",
    "}\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Fill Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"your_path\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Locations of the Stadiums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname='football_stadiums_2023.csv'\n",
    "df = pd.read_csv(path+fname, delimiter=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop([\"Division\", \"Stadium\"], axis = \"columns\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col_name in [\"Longitude\", \"Latitude\"]:\n",
    "    df = clean_col(df, col_name)\n",
    "    \n",
    "df = df.rename(columns={\n",
    "    'Longitude': 'longitude',\n",
    "    'Latitude': 'latitude'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.astype({ 'Team' : str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('datasets/laliga_rounded.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 La Liga fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.2.1 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname='La_Liga_Interactive_Table_2022-23_20230521.xlsm'\n",
    "df_liga = pd.read_excel(path+fname, sheet_name='fixtures')\n",
    "df_liga.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.2.2 Cleaning and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_liga = df_liga[['Match Time(CET)', 'Home Team']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_liga = df_liga.dropna(how='all') # remove NA, a few last lines were empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replacements = {'Real Sociedad':'Real Sociedad ', 'Elche CF': 'Elche Club de Futbol', 'RCD Mallorca': 'Majorque',  'RCD Espanyol' : 'Real Club Deportivo Espanyol','Getafe CF':'Getafe', 'UD Almería' :'Almeria','Cádiz CF': \"Cadix\", \"CA Osasuna\" : \"Osasuna\", 'Real Betis': 'Betis Seville', \"Girona FC\" : \"Gérone\", \"FC Barcelona\" : \"FC Barcelone\", \"RC Celta\" : \"Celta Vigo\", \"Villarreal CF\" : \"Villareal\", \"Athletic Club\": \"Athletic Bilbao\", \"Valencia CF\": \"Valence FC\", \"Real Valladolid CF\" : \"Real Valladolid Club de Futbol\", \"Sevilla FC\" : \"Séville FC\", \"Huesca\" : \"Sociedad Deportiva Huesca\" }\n",
    "\n",
    "df_liga['Home Team'] = df_liga['Home Team'].replace(replacements) # replace the name to have matching names between the 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_liga['Match Time(CET)'] = pd.to_datetime(df_liga['Match Time(CET)'], format='%Y-%m-%d')\n",
    "df_liga['Date'] = pd.to_datetime(df_liga['Match Time(CET)'].dt.date, format='%Y-%m-%d') # create a date column\n",
    "df_liga['Hour'] = df_liga['Match Time(CET)'].dt.hour + df_liga['Match Time(CET)'].dt.minute / 60 # create hour column\n",
    "df_liga[\"Month\"] = df_liga['Date'].dt.month\n",
    "df_liga[\"Day\"] = df_liga['Date'].dt.day\n",
    "df_liga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = (\n",
    "        (df_liga['Month'] == 6) |\n",
    "        (df_liga['Month'] == 7) |\n",
    "        (df_liga['Month'] == 8) |\n",
    "        (df_liga['Month'] == 9)\n",
    ")\n",
    "df_liga = df_liga.loc[mask]\n",
    "df_liga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_liga.to_csv(\"datasets/clean_laliga_2023.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.2.3 Create fake fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_liga = create_fake_features(df_liga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_liga.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_years_df = extend_years(df_liga, 2002, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_years_df = all_years_df.drop(columns=['Match Time(CET)']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_years_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_years_df.to_csv('datasets/created_fixtures_2002_2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 Merge fixtures and locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_liga = pd.read_csv(\"datasets/created_fixtures_2002_2023.csv\") # contains LaLiga summer fixtures for 2002 up to 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/laliga_rounded.csv') # contains longitude, latitude, and capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df = df_liga.merge(df, left_on='Home Team', right_on='Team', how='inner')\n",
    "merged_df.drop([\"Unnamed: 0_x\", \"Unnamed: 0_y\", \"Team\"], axis = 1, inplace= True)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"datasets/fixtures_locations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.4 fill UTCI historical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yearly_df = pd.read_csv(\"datasets/fixtures_locations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#folder_path = \"/Volumes/ASRNewVolume_805/dataset-derived-utci-historical-7d82619a-2530-4700-86d8-0f9fb3fd727b\"\n",
    "#folder_path = \"/Volumes/ASRNewVolume_805/dataset-derived-utci-historical-837c29ed-9853-4dc0-93a4-3f6c3b080b93\"\n",
    "#folder_path =\"/Volumes/ASRNewVolume_805/dataset-derived-utci-historical-8cf94634-325a-4a00-aab9-0e290b98ffef\"\n",
    "folder_path = \"/Volumes/ASRNewVolume_805/dataset-derived-utci-historical-486073aa-be98-4d08-bd86-276a57ede642\"\n",
    "nc_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.nc') ]\n",
    "datasets = {get_date(os.path.basename(f)): xr.open_dataset(f, engine='netcdf4') for f in nc_files} # creates dict, key = date of the dataset, value = content of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_date = set()\n",
    "\n",
    "for index, row in yearly_df.iterrows():\n",
    "    date = pd.to_datetime(row['Date'])\n",
    "    lat = row['latitude']\n",
    "    lon = row['longitude']\n",
    "    if date in datasets: \n",
    "        value = get_timeseries_at_lat_lon_time(datasets[date], lat, lon, row['valid_time'])\n",
    "        yearly_df.at[index, 'utci'] = value\n",
    "    else: # check missing dates \n",
    "        set_date.add(date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yearly_df = yearly_df.sort_values(by=['Date', 'Hour', 'Home Team'])\n",
    "yearly_df = yearly_df.reset_index(drop=True)\n",
    "yearly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yearly_df.to_csv('datasets/UTCI.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.5 Historical and rcp45 Projections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df = pd.read_csv(\"datasets/fixtures_locations.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df['Year'] = pd.to_datetime(result_df['Date']).dt.year\n",
    "result_df['timestamp'] = pd.to_datetime(result_df[['Year', 'Month', 'Day', 'Hour']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_folders = ['/Volumes/ASRNewVolume_805/historical', '/Volumes/ASRNewVolume_805/rcp45']\n",
    "for folder_path in root_folders:\n",
    "    folders = [f for f in os.listdir(folder_path) if f != \".DS_Store\"]\n",
    "    for f in folders:\n",
    "        result_df = fill_1_year(folder_path +\"/\"+ f, result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df.to_csv(\"datasets/full_temp_data_2002_2023.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.6 Rcp 4.5 projections 2024-2043"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.6.1 Load Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/laliga_rounded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"datasets/clean_laliga_2023.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.6.2 Create fake fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2['Match Time(CET)'] = pd.to_datetime(df2['Match Time(CET)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2_extended = create_fake_features(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2_extended[\"Date\"] = pd.to_datetime(df2_extended[\"Date\"], format = \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_years_df = extend_years(df2_extended, 2024, 2044)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_df = all_years_df\n",
    "cleaned_df = cleaned_df.drop(columns=['Match Time(CET)'])\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_df.to_csv('datasets/created_fixtures_2024_2044.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.6.3 Merge fixtures and locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_liga = pd.read_csv(\"datasets/created_fixtures_2024_2044.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/laliga_rounded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df = df_liga.merge(df, left_on='Home Team', right_on='Team', how='inner')\n",
    "merged_df.drop([\"Unnamed: 0_y\", \"Unnamed: 0_x\", \"Team\"], axis = 1, inplace = True)\n",
    "result_df['Year'] = pd.to_datetime(result_df['Date']).dt.year\n",
    "result_df['timestamp'] = pd.to_datetime(result_df[['Year', 'Month', 'Day', 'Hour']])\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"datasets/2024_2044fixtures_locations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.6.4 Historical and rcp45 Projections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_folders = [ '/Volumes/ASRNewVolume_805/rcp45_projections']\n",
    "for folder_path in root_folders:\n",
    "    folders = [f for f in os.listdir(folder_path) if f != \".DS_Store\"]\n",
    "\n",
    "    for f in folders:\n",
    "        result_df = fill_1_year(folder_path +\"/\"+ f, result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df.to_csv(\"datasets/full_temp_data_2024_2044_rcp45.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.7 Rcp 2.6 projections 2024-2043"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.7.1 Historical and rcp26 Projections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"datasets/2024_2044fixtures_locations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_folders = [ '/Volumes/ASRNewVolume_805/rcp26_projections']\n",
    "for folder_path in root_folders:\n",
    "    folders = [f for f in os.listdir(folder_path) if f != \".DS_Store\"]\n",
    "    for f in folders:\n",
    "        result_df = fill_1_year(folder_path +\"/\"+ f, result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df.to_csv(\"datasets/full_temp_data_2024_2044_rcp26.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.8 Rcp 8.5 projections 2024-2044"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"datasets/2024_2044fixtures_locations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.8.1 Historical and rcp26 Projections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_folders = [ '/Volumes/ASRNewVolume_805/rcp85_projections']\n",
    "for folder_path in root_folders:\n",
    "    folders = [f for f in os.listdir(folder_path) if f != \".DS_Store\"]\n",
    "    for f in folders:\n",
    "        result_df = fill_1_year(folder_path +\"/\"+ f, result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df.to_csv(\"datasets/full_temp_data_2024_2044_rcp85.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " ## 4. Calcul UTCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.1 UTCI 2002-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_past_proj = pd.read_csv(\"datasets/full_temp_data_2002_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_past_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_past_proj = calculate_UTCI(df_past_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_past_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_past_proj.to_csv(\"datasets/UTCI_2002_2023_proj_and_histo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.2 UTCI 2024-2044 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_45 = pd.read_csv(\"datasets/full_temp_data_2024_2044_rcp45.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_45  = calculate_UTCI(df_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_45.to_csv(\"datasets/UTCI_2024_2044_45.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.3 UTCI 2024-2044 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_26 = pd.read_csv(\"datasets/full_temp_data_2024_2044_rcp26.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_26  = calculate_UTCI(df_26)\n",
    "df_26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_26.to_csv(\"datasets/UTCI_2024_2044_26.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.4 UTCI 2024-2044 8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_85 = pd.read_csv(\"datasets/full_temp_data_2024_2044_rcp85.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_85 = calculate_UTCI(df_85)\n",
    "df_85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_85.to_csv(\"datasets/UTCI_2024_2044_85.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. Descriptive Analysis: What happened ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/UTCI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['utci'] = df['utci'] - 273.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['utci']) # few missing values, got removed\n",
    "df = df.sort_values(\"True_Game\", ascending= False).drop_duplicates([\"Home Team\", 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = categorize_stress(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[[\"Home Team\", \"timestamp\",\"True_Game\", \"Capacity\", 'utci', 'Stress Category']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"policies\"] = df[\"utci\"].apply(assign_authorities_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location_utci_stats = optimize_clusters(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location_utci_stats = location_utci_stats[[\"Home Team\", \"Optimal_Cluster\"]]\n",
    "df = pd.merge(df, location_utci_stats, on='Home Team', how='inner')\n",
    "df = df.sort_values(\"True_Game\", ascending= False).drop_duplicates([\"Home Team\", 'timestamp'])\n",
    "df = df[df[\"year\"] != 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"datasets/UTCI_cat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/UTCI_cat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.1 Locations of the fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_liga = pd.read_csv(\"clean_laliga_2023.csv\")\n",
    "plot_locations_histogram(df_liga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.2 Number of games per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hour_counts_sorted = df_liga['Hour'].value_counts().sort_index()\n",
    "print(hour_counts_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_liga['Hour'][df_liga['Hour'] == 22] = 21\n",
    "df_liga['Hour'][df_liga['Hour'] == 20] = 21\n",
    "df_liga['Hour'][df_liga['Hour'] == 19.5] = 21\n",
    "df_liga['Hour'][df_liga['Hour'] == 17.5] = 18\n",
    "df_liga['Hour'][df_liga['Hour'] == 21.5] = 21\n",
    "df_liga['Hour'][df_liga['Hour'] == 17] = 18\n",
    "df_liga['Hour'][df_liga['Hour'] == 19] = 18\n",
    "df_liga['Hour'][df_liga['Hour'] == 16] = 15\n",
    "df_liga['Hour'][df_liga['Hour'] == 14] = 15\n",
    "# 21 overlappings\n",
    "df_liga[[\"Hour\", \"month\", \"day\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.3 Heat conditions for summer afternoons in Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_overall_percentage_stress_category(df,'Percentage of Different Stress Categories for Available Timeslots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.4 Policies for summer afternoons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_overall_percentage_policies_category(df,'Percentage of Different Policies Categories for Available Timeslots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.5 Heat Stress conditions for LaLiga games in summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute the function with the DataFrame\n",
    "plot_overall_percentage_stress_category(df[df[\"True_Game\"] == 1],'Percentage of Different Stress Categories for Games Played')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_overall_percentage_policies_category(df[df[\"True_Game\"] == 1],'Percentage of Different Policies Categories for True Games')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.6 Heat stress conditions by locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_optimal_cluster_percentage_stress_category_histogram(df, 'Percentage Histogram of Stress Categories Grouped by Optimal Cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.7 Heat stress conditions by locations during games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_optimal_cluster_percentage_stress_category_histogram(df[df[\"True_Game\"] == 1], 'Percentage Histogram of Stress Categories during Games Grouped by Optimal Cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.8 Heat Stress conditions by hour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_grouped_percentage_stress_category_histogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.9 Heat stress conditions by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_grouped_month_percentage_stress_category_histogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5.10 Exploratory analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[[\"Date\", \"True_Game\", \"Capacity\", \"utci\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting UTCI vs Year with different lines for each group\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.lineplot(x='year', y='utci', hue='Optimal_Cluster', data=df, marker=\"o\")\n",
    "\n",
    "plt.title('UTCI vs Year by Groups')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('UTCI')\n",
    "plt.legend(title='Groups', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 5.10.1 UTCI distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating a histogram of the 'utci' column\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['utci'], kde=False, color='blue')\n",
    "plt.title('Histogram of UTCI Values')\n",
    "plt.xlabel('UTCi')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(False)\n",
    "plt.gca().set_facecolor('white')\n",
    "\n",
    "# Saving the histogram to a file\n",
    "histogram_file_path = 'plots/past_utci_histogram.png'\n",
    "plt.savefig(histogram_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "####  5.10.2 Boxplots by locations , optimal cluster highlighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(['Optimal_Cluster', 'Home Team'])\n",
    "\n",
    "# Setting up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Home Team', y='utci', hue='Optimal_Cluster', data=df, palette='Set2')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('UTCI Values Colored by Optimal Cluster')\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.gca().set_facecolor('white')\n",
    "\n",
    "# Saving the plot\n",
    "file_path = 'plots/past_utci_boxplot_by_optimal_cluster.png'\n",
    "plt.savefig(file_path)\n",
    "\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6. Prescriptive Analysis: What will happen ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.1 Data merging and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_85 = pd.read_csv(\"datasets/UTCI_2024_2044_85.csv\")\n",
    "df_45 = pd.read_csv('datasets/UTCI_2024_2044_45.csv')\n",
    "df_26 = pd.read_csv('datasets/UTCI_2024_2044_26.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_85.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_85 = df_85[[\"Home Team\", \"timestamp\", \"True_Game\", \"UTCI\"]]\n",
    "df_85.rename(columns={'UTCI': 'UTCI_85'}, inplace=True)\n",
    "df_45 = df_45[[\"Home Team\", \"timestamp\", \"True_Game\", \"UTCI\"]]\n",
    "df_45.rename(columns={'UTCI': 'UTCI_45'}, inplace=True)\n",
    "df_26.rename(columns={'UTCI': 'UTCI_26'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merging_df = pd.merge(df_26, df_45, how = 'inner', on = ['Home Team', \"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merging_df = pd.merge(merging_df, df_85, how = 'inner', on = ['Home Team', \"timestamp\"])\n",
    "merging_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.2 Download of the bias corrected version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_df = pd.read_csv(\"datasets/corrected_future_UTCI.csv\")\n",
    "future_df = future_df.rename(columns={\n",
    "    'utci_bc_rcp26': 'UTCI_26',\n",
    "    'utci_bc_rcp45': 'UTCI_45',\n",
    "    'utci_bc_rcp85': 'UTCI_85'\n",
    "})\n",
    "# Convert 'timestamp' to datetime for consistent comparison\n",
    "future_df[\"timestamp\"] = pd.to_datetime(future_df['timestamp'])\n",
    "future_df[\"hour\"] = pd.to_datetime(future_df['timestamp']).dt.hour\n",
    "future_df[\"month\"] = pd.to_datetime(future_df['timestamp']).dt.month\n",
    "future_df[\"day\"] = pd.to_datetime(future_df['timestamp']).dt.day\n",
    "future_df[\"year\"] = pd.to_datetime(future_df['timestamp']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# corrects issues whith True_Game\n",
    "rows_to_remove = pd.DataFrame()\n",
    "\n",
    "for year in future_df[\"year\"].unique():\n",
    "    # Update the \"Hour\" field for specific conditions\n",
    "    future_df.loc[(future_df[\"Home Team\"] == \"Betis Seville\") &\n",
    "                  (future_df[\"timestamp\"] == pd.Timestamp(f\"{year}-08-15 21:30:00\")), \"hour\"] = 22\n",
    "\n",
    "    future_df.loc[(future_df[\"Home Team\"] == \"Valence FC\") &\n",
    "                  (future_df[\"timestamp\"] == pd.Timestamp(f\"{year}-08-14 19:30:00\")), \"hour\"] = 20\n",
    "\n",
    "    future_df.loc[(future_df[\"Home Team\"] == \"Getafe\") &\n",
    "                  (future_df[\"timestamp\"] == pd.Timestamp(f\"{year}-08-15 19:30:00\")), \"hour\"] = 20\n",
    "\n",
    "    future_df.loc[(future_df[\"Home Team\"] == \"Majorque\") &\n",
    "                  (future_df[\"timestamp\"] == pd.Timestamp(f\"{year}-08-20 19:30:00\")), \"hour\"] = 20\n",
    "\n",
    "    future_df.loc[(future_df[\"Home Team\"] == \"Atlético de Madrid\") &\n",
    "                  (future_df[\"timestamp\"] == pd.Timestamp(f\"{year}-08-21 19:30:00\")), \"hour\"] = 20\n",
    "\n",
    "    future_df.loc[(future_df[\"Home Team\"] == \"Rayo Vallecano\") &\n",
    "                  (future_df[\"timestamp\"] == pd.Timestamp(f\"{year}-08-27 19:30:00\")), \"hour\"] = 20\n",
    "\n",
    "    future_df.loc[(future_df[\"Home Team\"] == \"FC Barcelone\") &\n",
    "                  (future_df[\"timestamp\"] == pd.Timestamp(f\"{year}-08-28 19:30:00\")), \"hour\"] = 20\n",
    "\n",
    "    # For \"Cadix\"\n",
    "    condition1 = future_df['Home Team'] == \"Cadix\"\n",
    "    condition2 = future_df['timestamp'] == pd.Timestamp(f\"{year}-08-14 18:00:00\")\n",
    "    rows_to_remove = rows_to_remove.append(future_df[condition1 & condition2])\n",
    "    future_df.loc[(condition1) & (pd.to_datetime(future_df[\"timestamp\"]) == pd.to_datetime(f\"{year}-08-14 17:30:00\")), \"hour\"] = 18\n",
    "\n",
    "    # For \"Athletic Bilbao\"\n",
    "    condition1 = future_df['Home Team'] == \"Athletic Bilbao\"\n",
    "    condition2 = future_df['timestamp'] == pd.Timestamp(f\"{year}-08-15 18:00:00\")\n",
    "    rows_to_remove = rows_to_remove.append(future_df[condition1 & condition2])\n",
    "    future_df.loc[(condition1) & (pd.to_datetime(future_df[\"timestamp\"]) == pd.to_datetime(f\"{year}-08-15 17:30:00\")), \"hour\"] = 18\n",
    "\n",
    "    condition2 = future_df['timestamp'] == pd.Timestamp(f\"{year}-08-20 18:00:00\")\n",
    "    rows_to_remove = rows_to_remove.append(future_df[condition1 & condition2])\n",
    "    future_df.loc[(condition1) & (pd.to_datetime(future_df[\"timestamp\"]) == pd.to_datetime(f\"{year}-08-20 17:30:00\")), \"hour\"] = 18\n",
    "\n",
    "    condition2 = future_df['timestamp'] == pd.Timestamp(f\"{year}-08-21 18:00:00\")\n",
    "    rows_to_remove = rows_to_remove.append(future_df[condition1 & condition2])\n",
    "    future_df.loc[(condition1) & (pd.to_datetime(future_df[\"timestamp\"]) == pd.to_datetime(f\"{year}-08-21 17:30:00\")), \"hour\"] = 18\n",
    "\n",
    "    # For \"Osasuna\"\n",
    "    condition1 = future_df['Home Team'] == \"Osasuna\"\n",
    "    rows_to_remove = rows_to_remove.append(future_df[condition1 & condition2])\n",
    "    future_df.loc[(condition1) & (pd.to_datetime(future_df[\"timestamp\"]) == pd.to_datetime(f\"{year}-08-20 17:30:00\")), \"hour\"] = 18\n",
    "    \n",
    "    #For \"Elche Club de Futbol\"\n",
    "    condition1 = future_df['Home Team'] == \"Elche Club de Futbol\"\n",
    "    condition2 = future_df['timestamp'] == pd.Timestamp(f\"{year}-08-27 18:00:00\")\n",
    "    rows_to_remove = rows_to_remove.append(future_df[condition1 & condition2])\n",
    "    future_df.loc[(condition1) & (pd.to_datetime(future_df[\"timestamp\"]) == pd.to_datetime(f\"{year}-08-27 17:30:00\")), \"hour\"] = 18\n",
    "\n",
    "    #For \"Getafe\"\n",
    "    condition1 = future_df['Home Team'] == \"Getafe\"\n",
    "    condition2 = future_df['timestamp'] == pd.Timestamp(f\"{year}-08-28 18:00:00\")\n",
    "    rows_to_remove = rows_to_remove.append(future_df[condition1 & condition2])\n",
    "    future_df.loc[(condition1) & (pd.to_datetime(future_df[\"timestamp\"]) == pd.to_datetime(f\"{year}-08-28 17:30:00\")), \"hour\"] = 18\n",
    "\n",
    "# Drop the collected rows from `future_df`\n",
    "if not rows_to_remove.empty:\n",
    "    future_df = future_df.drop(rows_to_remove.index).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_df = future_df.sort_values([\"True_Game\"], ascending = False)\n",
    "future_df = future_df.drop_duplicates([\"Home Team\", \"month\", 'day', 'hour', 'year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "future_df.sort_values(['Home Team', \"month\", \"day\", \"hour\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.2.1 compare data before and after bias correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "before_bias_correction = merging_df\n",
    "after_bias_correction = future_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the palette for the different datasets\n",
    "palette = [\"#1f77b4\", \"#ff7f0e\"]  # You can choose different colors\n",
    "\n",
    "# Set the style to 'white' which removes the grid and has a white background\n",
    "sns.set_style(\"white\")\n",
    "# Prepare the data for plotting\n",
    "data_to_plot = pd.concat([\n",
    "    before_bias_correction.melt(value_vars=['UTCI_26', 'UTCI_45', 'UTCI_85'], var_name='Variable', value_name='Value').assign(Dataset='before_bias_correction'),\n",
    "    after_bias_correction.melt(value_vars=['UTCI_26', 'UTCI_45', 'UTCI_85'], var_name='Variable', value_name='Value').assign(Dataset='after_bias_correction')\n",
    "])\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Variable', y='Value', hue='Dataset', data=data_to_plot, palette=palette)\n",
    "plt.title('Box plot of UTCI_26, UTCI_45, and UTCI_85 from before and after bias correction')\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(title='Dataset')\n",
    "plt.savefig(\"plots/bias_correction.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_df = future_df[[\"Home Team\", \"True_Game\", \"Stadium\", \"latitude\", \"longitude\", \"Capacity\", \"Division\", \"timestamp\", \"UTCI_26\", \"UTCI_45\", \"UTCI_85\", 'hour', 'month','day', 'year']]\n",
    "future_df = categorize_stress_future(future_df, \"UTCI_26\", \"Stress Category 26\")\n",
    "future_df = categorize_stress_future(future_df, \"UTCI_45\", \"Stress Category 45\")\n",
    "future_df = categorize_stress_future(future_df, \"UTCI_85\", \"Stress Category 85\")\n",
    "#merging_df.to_csv(\"full_UTCI_comparison_2024_2044.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "na_count = future_df['UTCI_26'].isna().sum()\n",
    "print(f\"Number of NA values in the column: {na_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_df = future_df.dropna(subset=['UTCI_26']) # remove pcq sinon t'as des NA dans les percenatges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(future_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for scenario in [\"26\", \"45\", \"85\"]:\n",
    "    future_df[f'policies_rcp{scenario}'] = future_df[f\"UTCI_{scenario}\"].apply(assign_authorities_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_df = pd.merge(future_df, location_utci_stats, on='Home Team', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_df.to_csv(\"datasets/full_UTCI_comparison_2024_2044_corrected.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_df = pd.read_csv(\"datasets/full_UTCI_comparison_2024_2044_corrected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_df[[\"True_Game\", \"Capacity\", \"UTCI_26\", \"UTCI_45\", \"UTCI_85\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6.3 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of columns to plot\n",
    "stress_columns = ['Stress Category 26', 'Stress Category 45', 'Stress Category 85']\n",
    "\n",
    "# Call the function with the DataFrame and the list of stress category columns\n",
    "plot_stress_category_distribution_colored(future_df, stress_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "future_df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example usage of the function\n",
    "plot_stress_category_distribution_with_values(future_df,  'Stress Category 26', 'UTCI 2.6 Stress Category Distribution by Month')\n",
    "plot_stress_category_distribution_with_values(future_df, 'Stress Category 45', 'UTCI 4.5 Stress Category Distribution by Month')\n",
    "plot_stress_category_distribution_with_values(future_df,  'Stress Category 85', 'UTCI 8.5 Stress Category Distribution by Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example usage of the function for UTCI categories by hour\n",
    "plot_stress_category_distribution_by_hour(future_df, 'UTCI_26', 'Stress Category 26', 'UTCI 2.6 Stress Category Distribution by Hour')\n",
    "plot_stress_category_distribution_by_hour(future_df, 'UTCI_45', 'Stress Category 45', 'UTCI 4.5 Stress Category Distribution by Hour')\n",
    "plot_stress_category_distribution_by_hour(future_df, 'UTCI_85', 'Stress Category 85', 'UTCI 8.5 Stress Category Distribution by Hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "order = ['No thermal stress', 'Moderate heat stress', 'Strong heat stress',\n",
    "         'Very strong heat stress', 'Extreme heat stress', 'Slight cold stress']\n",
    "# Generating and saving the plots for each RCP with colors\n",
    "plot_and_save_stress_category_percentage_ordered(future_df, 'Stress Category 26', '2.6', \"plots/rcp26_clusters.pdf\", color_mapping, order)\n",
    "plot_and_save_stress_category_percentage_ordered(future_df, 'Stress Category 45', '4.5', \"plots/rcp45_clusters.pdf\", color_mapping, order)\n",
    "plot_and_save_stress_category_percentage_ordered(future_df, 'Stress Category 85', '8.5', \"plots/rcp85_clusters.pdf\", color_mapping, order)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Optimization Problem"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.1 RCP 8.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/full_UTCI_comparison_2024_2044_corrected.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"Home Team\"] = df[\"Home Team\"].apply(remove_accents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "temp = df[[\"Home Team\",  \"hour\", \"day\", \"month\"]].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df[\"True_Game\"] == 1][[\"Home Team\", \"month\", \"day\", \"hour\", 'True_Game']].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['hour'][df['hour'] == 22] = 21\n",
    "df['hour'][df['hour'] == 20] = 21\n",
    "df['hour'][df['hour'] == 17] = 18\n",
    "df['hour'][df['hour'] == 19] = 18\n",
    "df['hour'][df['hour'] == 16] = 15\n",
    "df['hour'][df['hour'] == 14] = 15\n",
    "\n",
    "df_temp = df[df[\"True_Game\"] == 1][[\"Home Team\", \"month\", \"day\", \"hour\", 'True_Game']].drop_duplicates()\n",
    "nb_by_hour = pd.DataFrame()\n",
    "nb_by_hour[\"hour\"] = df_temp[\"hour\"].drop_duplicates()\n",
    "nb_by_hour['nb_of_occurences']= df_temp['hour'].value_counts().values\n",
    "nb_by_hour\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_copy = df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert 'timestamp' to datetime and extract date and hour\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "# Create a mapping of stress categories to numerical values\n",
    "stress_value_map = {\n",
    "    'No thermal stress': 0,\n",
    "    'Moderate heat stress': 1,\n",
    "    'Strong heat stress': 2,\n",
    "    'Very strong heat stress': 3,\n",
    "    'Extreme heat stress': 4\n",
    "}\n",
    "\n",
    "# Map the stress categories in 'Stress Category 85' to their numerical values\n",
    "df['Stress Value 85'] = df['Stress Category 85'].map(stress_value_map)\n",
    "\n",
    "# Group by location (latitude, longitude), date, and hour, and calculate the sum of average cumulated percentages\n",
    "r_df = df.groupby(['Home Team', 'month','hour'])['Stress Value 85'].mean().reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "r_df.rename(columns={'Stress Value 85': 'Average Stress Value 85'}, inplace=True)\n",
    "\n",
    "r_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime format to extract day and month\n",
    "unique_combinations_df = df_copy\n",
    "unique_combinations_df['date'] = pd.to_datetime(unique_combinations_df['timestamp'])\n",
    "unique_combinations_df['day'] = unique_combinations_df['date'].dt.day\n",
    "unique_combinations_df['month'] = unique_combinations_df['date'].dt.month\n",
    "\n",
    "# Drop the 'date' column as we have separated it into 'day' and 'month'\n",
    "unique_combinations_df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "unique_combinations_df.drop_duplicates(inplace=True)\n",
    "\n",
    "x_df = unique_combinations_df[[\"Home Team\", \"day\", \"month\", \"hour\"]]\n",
    "x_df.drop_duplicates(inplace=True)\n",
    "x_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "o_df = x_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_short = df[[\"Home Team\", \"Capacity\"]].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_short"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dictionary with keys as home team and values as the capacity\n",
    "capacities_dict = df_short.set_index('Home Team')['Capacity'].to_dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = gp.Model(\"linear_problem\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "o_df['key'] = list(zip(o_df[\"Home Team\"], o_df[\"month\"], o_df[\"day\"], o_df[\"hour\"]))\n",
    "x_df[\"key\"] = list(zip(x_df[\"Home Team\"], x_df[\"month\"], x_df[\"day\"], x_df[\"hour\"]))\n",
    "\n",
    "# Extract the unique keys\n",
    "unique_keys_o = o_df['key'].unique()\n",
    "unique_keys_x = x_df[\"key\"].unique()\n",
    "\n",
    "\n",
    "# Create binary variables o_d_l using unique keys\n",
    "x_h_d_l = model.addVars(unique_keys_x, vtype=GRB.BINARY, name=\"x\")\n",
    "o_d_l = model.addVars(unique_keys_o, vtype=GRB.BINARY, name=\"o\")\n",
    "\n",
    "\n",
    "\n",
    "value_column_name = r_df.columns[-1]\n",
    "# Create a dictionary for the values\n",
    "values_dict = {(row['hour'], row['month'], row['Home Team']): row[value_column_name]\n",
    "               for index, row in r_df.iterrows()}\n",
    "model.update()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to hold the mapping\n",
    "team_month_day_hours = {}\n",
    "\n",
    "# Iterate through the DataFrame to populate the dictionary\n",
    "for index, row in df.iterrows():\n",
    "    home_team = row['Home Team']\n",
    "    day = row[\"day\"]\n",
    "    month = row[\"month\"]\n",
    "    hour = row['hour']\n",
    "\n",
    "    # Initialize the nested dictionaries if the 'Home Team' or month or day is not already in the dictionary\n",
    "    if home_team not in team_month_day_hours:\n",
    "        team_month_day_hours[home_team] = {}\n",
    "    if month not in team_month_day_hours[home_team]:\n",
    "        team_month_day_hours[home_team][month] = {}\n",
    "    if day not in team_month_day_hours[home_team][month]:\n",
    "        team_month_day_hours[home_team][month][day] = []\n",
    "\n",
    "    # Append the hour to the list of hours available for that 'Home Team', month, and day\n",
    "    if hour not in team_month_day_hours[home_team][month][day]:\n",
    "        team_month_day_hours[home_team][month][day].append(hour)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "unique_keys_x_list = unique_keys_x.tolist()\n",
    "for home_team, months in team_month_day_hours.items():\n",
    "    for month, days in months.items():\n",
    "        for day, hours_list in days.items():\n",
    "            # Ensure that home_team, month, and day are in the correct type before comparison\n",
    "            home_team_str = str(home_team)  \n",
    "            month_int = int(month)\n",
    "            day_int = int(day)\n",
    "\n",
    "            vars_in_constraint = [x_h_d_l[home_team_str, month_int, day_int, h] for h in hours_list if (home_team_str, month_int, day_int, h) in unique_keys_x_list]\n",
    "            model.addConstr(sum(vars_in_constraint) == 1, \"daily_hour_sum_constraint_{}_{}_{}\".format(home_team, month, day))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for hour in nb_by_hour[\"hour\"]:\n",
    "    print(hour, nb_by_hour[nb_by_hour['hour'] == hour]['nb_of_occurences'].values[0])\n",
    "    sum_x_vars = sum(x_h_d_l[l, m, d, hour] for l, m, d, h in x_h_d_l.keys() if h == hour)\n",
    "    model.addConstr(sum_x_vars == nb_by_hour[nb_by_hour['hour'] == hour]['nb_of_occurences'].values[0], f\"hour_constraint_{hour}\")\n",
    "    \n",
    "# Define a large M value\n",
    "M = 1000\n",
    "\n",
    "# Iterate over each unique combination of location, month, day, and hour\n",
    "for l, month_days in team_month_day_hours.items():\n",
    "    for m, days in month_days.items():\n",
    "        for d, hours_list in days.items():\n",
    "            for h in hours_list:\n",
    "                sum_x_vars = sum(x_h_d_l[l2, m, d, h] for l2 in team_month_day_hours if (l2, m, d, h) in unique_keys_x_list)\n",
    "            \n",
    "                # Constraint 1: If two or more games are scheduled, overlapping must be 1\n",
    "                model.addConstr(sum_x_vars <= 1 + M * o_d_l[l, m, d, h],\n",
    "                                f\"overlap_max_constraint_{l}_{m}_{d}_{h}\")\n",
    "                \n",
    "                # Constraint 2: If less than two games are scheduled, overlapping must be 0\n",
    "                model.addConstr(sum_x_vars >= 2 - M * (1 - o_d_l[l, m, d, h]),\n",
    "                                f\"overlap_min_constraint_{l}_{m}_{d}_{h}\")\n",
    "                \n",
    "model.update()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the objective function as a linear expression because r_h_m_l are constants\n",
    "objective = gp.LinExpr()\n",
    "\n",
    "i = 0\n",
    "# Add the sum of products of x_h_d_l and r_h_m_l for matching hour, location, and month\n",
    "for x_key in x_h_d_l.keys():\n",
    "    \n",
    "    \n",
    "    l, m, d, h = x_key  \n",
    "    r_key = (h, m, l)\n",
    "    if r_key in values_dict:  # values_dict contains fixed values for r_h_m_l\n",
    "        i += 1\n",
    "        objective.add(x_h_d_l[x_key] * values_dict[r_key] * 0.99995)\n",
    "       \n",
    "# Add the sum of products of o_d_l and c_l for matching location\n",
    "for o_key in o_d_l.keys():\n",
    "    \n",
    "    l,d,m,h = o_key  # o_key is a tuple (d, l)\n",
    "    if l in capacities_dict:  # capacities_dict contains the capacities corresponding to c_l\n",
    "        i+=1\n",
    "        objective.add(o_d_l[o_key] * capacities_dict[l] * 0.00005)\n",
    "\n",
    "\n",
    "# Set the objective function in the model\n",
    "model.update()\n",
    "model.setObjective(objective, GRB.MINIMIZE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_terms = objective.size()\n",
    "print(\"Number of terms in the objective function:\", num_terms)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Optimize the model\n",
    "model.optimize()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List to hold the data\n",
    "data = []\n",
    "\n",
    "# Check if the solution exists\n",
    "if model.status == GRB.OPTIMAL:\n",
    "    # Iterate over the variables\n",
    "    for v in model.getVars():\n",
    "        # Check if the variable value is 1\n",
    "        if round(v.x) == 1:\n",
    "            # Parse the variable name\n",
    "            print(v.varName.split(','))\n",
    "            team, month, day, hour = v.varName.split(',')\n",
    "            # Add to the list\n",
    "            if team[0] == 'x':\n",
    "                team = team[2:]\n",
    "                hour = hour[:-1]\n",
    "                data.append([team, int(month), int(day), int(hour)])\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df_res = pd.DataFrame(data, columns=['Team', 'Month', 'Day', 'Hour'])\n",
    "else:\n",
    "    print(\"No optimal solution found.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_res[\"optimal_schedule\"] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df = pd.merge(df, df_res, left_on=[\"Home Team\", \"hour\", \"month\", \"day\"], right_on=[\"Team\", \"Hour\", \"Month\", \"Day\"], how=\"inner\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_stress_category(merge_df,'Percentage of Different Stress Categories for optimal game features', 'Stress Category 85')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_stress_category(df[df[\"True_Game\"] == 1],'Percentage of Different Stress Categories for non optimal game features', 'Stress Category 85')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_policies_category(merge_df,'Percentage of Different Policies Categories for Available Timeslots', \"policies_rcp85\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_policies_category(df[df[\"True_Game\"] == 1],'Percentage of Different Policies Categories for Available Timeslots', \"policies_rcp85\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_policies_category(df,'Percentage of Different Policies Categories for Available Timeslots', \"policies_rcp85\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.2 RCP 4.5 "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/full_UTCI_comparison_2024_2044_corrected.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"Home Team\"] = df[\"Home Team\"].apply(remove_accents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "temp = df[[\"Home Team\",  \"hour\", \"day\", \"month\"]].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df[\"True_Game\"] == 1][[\"Home Team\", \"month\", \"day\", \"hour\", 'True_Game']].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['hour'][df['hour'] == 22] = 21\n",
    "df['hour'][df['hour'] == 20] = 21\n",
    "df['hour'][df['hour'] == 17] = 18\n",
    "df['hour'][df['hour'] == 19] = 18\n",
    "df['hour'][df['hour'] == 16] = 15\n",
    "df['hour'][df['hour'] == 14] = 15\n",
    "\n",
    "df_temp = df[df[\"True_Game\"] == 1][[\"Home Team\", \"month\", \"day\", \"hour\", 'True_Game']].drop_duplicates()\n",
    "nb_by_hour = pd.DataFrame()\n",
    "nb_by_hour[\"hour\"] = df_temp[\"hour\"].drop_duplicates()\n",
    "nb_by_hour['nb_of_occurences']= df_temp['hour'].value_counts().values\n",
    "nb_by_hour\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_copy = df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert 'timestamp' to datetime and extract date and hour\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "# Create a mapping of stress categories to numerical values\n",
    "stress_value_map = {\n",
    "    'No thermal stress': 0,\n",
    "    'Moderate heat stress': 1,\n",
    "    'Strong heat stress': 2,\n",
    "    'Very strong heat stress': 3,\n",
    "    'Extreme heat stress': 4\n",
    "}\n",
    "\n",
    "# Map the stress categories in 'Stress Category 4.5' to their numerical values\n",
    "df['Stress Value 45'] = df['Stress Category 45'].map(stress_value_map)\n",
    "# Group by location (latitude, longitude), date, and hour, and calculate the sum of average cumulated percentages\n",
    "r_df = df.groupby(['Home Team', 'month','hour'])['Stress Value 45'].mean().reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "r_df.rename(columns={'Stress Value 45': 'Average Stress Value 45'}, inplace=True)\n",
    "\n",
    "r_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime format to extract day and month\n",
    "unique_combinations_df = df_copy\n",
    "unique_combinations_df['date'] = pd.to_datetime(unique_combinations_df['timestamp'])\n",
    "unique_combinations_df['day'] = unique_combinations_df['date'].dt.day\n",
    "unique_combinations_df['month'] = unique_combinations_df['date'].dt.month\n",
    "\n",
    "# Drop the 'date' column as we have separated it into 'day' and 'month'\n",
    "unique_combinations_df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "unique_combinations_df.drop_duplicates(inplace=True)\n",
    "\n",
    "x_df = unique_combinations_df[[\"Home Team\", \"day\", \"month\", \"hour\"]]\n",
    "x_df.drop_duplicates(inplace=True)\n",
    "x_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "o_df = x_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_short = df[[\"Home Team\", \"Capacity\"]].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_short"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dictionary with keys as home team and values as the capacity\n",
    "capacities_dict = df_short.set_index('Home Team')['Capacity'].to_dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = gp.Model(\"linear_problem\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "o_df['key'] = list(zip(o_df[\"Home Team\"], o_df[\"month\"], o_df[\"day\"], o_df[\"hour\"]))\n",
    "x_df[\"key\"] = list(zip(x_df[\"Home Team\"], x_df[\"month\"], x_df[\"day\"], x_df[\"hour\"]))\n",
    "\n",
    "# Extract the unique keys\n",
    "unique_keys_o = o_df['key'].unique()\n",
    "unique_keys_x = x_df[\"key\"].unique()\n",
    "\n",
    "\n",
    "# Create binary variables o_d_l using unique keys\n",
    "x_h_d_l = model.addVars(unique_keys_x, vtype=GRB.BINARY, name=\"x\")\n",
    "o_d_l = model.addVars(unique_keys_o, vtype=GRB.BINARY, name=\"o\")\n",
    "\n",
    "value_column_name = r_df.columns[-1]\n",
    "# Create a dictionary for the values\n",
    "values_dict = {(row['hour'], row['month'], row['Home Team']): row[value_column_name]\n",
    "               for index, row in r_df.iterrows()}\n",
    "\n",
    "print(\"vdict\",values_dict)\n",
    "model.update()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to hold the mapping\n",
    "team_month_day_hours = {}\n",
    "\n",
    "# Iterate through the DataFrame to populate the dictionary\n",
    "for index, row in df.iterrows():\n",
    "    home_team = row['Home Team']\n",
    "    day = row[\"day\"]\n",
    "    month = row[\"month\"]\n",
    "    hour = row['hour']\n",
    "\n",
    "    # Initialize the nested dictionaries if the 'Home Team' or month or day is not already in the dictionary\n",
    "    if home_team not in team_month_day_hours:\n",
    "        team_month_day_hours[home_team] = {}\n",
    "    if month not in team_month_day_hours[home_team]:\n",
    "        team_month_day_hours[home_team][month] = {}\n",
    "    if day not in team_month_day_hours[home_team][month]:\n",
    "        team_month_day_hours[home_team][month][day] = []\n",
    "\n",
    "    # Append the hour to the list of hours available for that 'Home Team', month, and day\n",
    "    if hour not in team_month_day_hours[home_team][month][day]:\n",
    "        team_month_day_hours[home_team][month][day].append(hour)\n",
    "\n",
    "print('full dict', team_month_day_hours)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we can use this mapping to add constraints to the model\n",
    "\n",
    "# Now we can use this mapping to add constraints to the model\n",
    "unique_keys_x_list = unique_keys_x.tolist()\n",
    "for home_team, months in team_month_day_hours.items():\n",
    "    for month, days in months.items():\n",
    "        for day, hours_list in days.items():\n",
    "            # Ensure that home_team, month, and day are in the correct type before comparison\n",
    "            home_team_str = str(home_team)  # or int(home_team) based on your data\n",
    "            month_int = int(month)\n",
    "            day_int = int(day)\n",
    "\n",
    "            vars_in_constraint = [x_h_d_l[home_team_str, month_int, day_int, h] for h in hours_list if (home_team_str, month_int, day_int, h) in unique_keys_x_list]\n",
    "            #print(f\"Constraint for {home_team} on {month}/{day} includes variables: {vars_in_constraint}\")\n",
    "            model.addConstr(sum(vars_in_constraint) == 1, \"daily_hour_sum_constraint_{}_{}_{}\".format(home_team, month, day))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for hour in nb_by_hour[\"hour\"]:\n",
    "    print(hour, nb_by_hour[nb_by_hour['hour'] == hour]['nb_of_occurences'].values[0])\n",
    "    sum_x_vars = sum(x_h_d_l[l, m, d, hour] for l, m, d, h in x_h_d_l.keys() if h == hour)\n",
    "    model.addConstr(sum_x_vars == nb_by_hour[nb_by_hour['hour'] == hour]['nb_of_occurences'].values[0], f\"hour_constraint_{hour}\")\n",
    "    \n",
    "# Define a large M value\n",
    "M = 1000\n",
    "\n",
    "# Iterate over each unique combination of location, month, day, and hour\n",
    "for l, month_days in team_month_day_hours.items():\n",
    "    for m, days in month_days.items():\n",
    "        for d, hours_list in days.items():\n",
    "            for h in hours_list:\n",
    "                sum_x_vars = sum(x_h_d_l[l2, m, d, h] for l2 in team_month_day_hours if (l2, m, d, h) in unique_keys_x_list)\n",
    "            \n",
    "                # Constraint 1: If two or more games are scheduled, overlapping must be 1\n",
    "                model.addConstr(sum_x_vars <= 1 + M * o_d_l[l, m, d, h],\n",
    "                                f\"overlap_max_constraint_{l}_{m}_{d}_{h}\")\n",
    "                \n",
    "                # Constraint 2: If less than two games are scheduled, overlapping must be 0\n",
    "                model.addConstr(sum_x_vars >= 2 - M * (1 - o_d_l[l, m, d, h]),\n",
    "                                f\"overlap_min_constraint_{l}_{m}_{d}_{h}\")\n",
    "                \n",
    "model.update()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the objective function as a linear expression because r_h_m_l are constants\n",
    "objective = gp.LinExpr()\n",
    "\n",
    "i = 0\n",
    "# Add the sum of products of x_h_d_l and r_h_m_l for matching hour, location, and month\n",
    "for x_key in x_h_d_l.keys():\n",
    "    \n",
    "    \n",
    "    l, m, d, h = x_key \n",
    "    r_key = (h, m, l)\n",
    "\n",
    "    if r_key in values_dict:  # values_dict contains fixed values for r_h_m_l\n",
    "        \n",
    "        i += 1\n",
    "        objective.add(x_h_d_l[x_key] * values_dict[r_key] * 0.99995)    \n",
    "# Add the sum of products of o_d_l and c_l for matching location\n",
    "for o_key in o_d_l.keys():\n",
    "    \n",
    "    l,d,m,h = o_key  # o_key is a tuple (d, l)\n",
    "    if l in capacities_dict:  # capacities_dict contains the capacities corresponding to c_l\n",
    "        i+=1\n",
    "        objective.add(o_d_l[o_key] * capacities_dict[l] * 0.00005)\n",
    "\n",
    "\n",
    "# Set the objective function in the model\n",
    "model.update()\n",
    "model.setObjective(objective, GRB.MINIMIZE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_terms = objective.size()\n",
    "print(\"Number of terms in the objective function:\", num_terms)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print all constraints safely, handling potential encoding errors\n",
    "print(\"\\nConstraints:\")\n",
    "constraints = model.getConstrs()\n",
    "for constr in constraints:\n",
    "    lhs_expression = model.getRow(constr)\n",
    "    rhs_value = constr.RHS\n",
    "    print(f\"{constr.ConstrName}: {lhs_expression} = {rhs_value}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Optimize the model\n",
    "model.optimize()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if model.status == GRB.OPTIMAL:\n",
    "    # Retrieve the values of the variables\n",
    "    for v in model.getVars():\n",
    "        print(f'{v.varName}: {v.x}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List to hold the data\n",
    "data = []\n",
    "\n",
    "# Check if the solution exists\n",
    "if model.status == GRB.OPTIMAL:\n",
    "    # Iterate over the variables\n",
    "    for v in model.getVars():\n",
    "        # Check if the variable value is 1\n",
    "        if round(v.x) == 1:\n",
    "            # Parse the variable name\n",
    "            print(v.varName.split(','))\n",
    "            team, month, day, hour = v.varName.split(',')\n",
    "            # Add to the list\n",
    "            if team[0] == 'x':\n",
    "                team = team[2:]\n",
    "                hour = hour[:-1]\n",
    "                data.append([team, int(month), int(day), int(hour)])\n",
    "        \n",
    "\n",
    "    # Create a DataFrame\n",
    "    df_res = pd.DataFrame(data, columns=['Team', 'Month', 'Day', 'Hour'])\n",
    "else:\n",
    "    print(\"No optimal solution found.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_res[\"optimal_schedule\"] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df = pd.merge(df, df_res, left_on=[\"Home Team\", \"hour\", \"month\", \"day\"], right_on=[\"Team\", \"Hour\", \"Month\", \"Day\"], how=\"inner\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df[\"Stress Category 45\"].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_stress_category(merge_df,'Percentage of Different Stress Categories for optimal game features', 'Stress Category 45')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_stress_category(df[df[\"True_Game\"] == 1],'Percentage of Different Stress Categories for non optimal game features', 'Stress Category 45')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_policies_category(merge_df,'Percentage of Different Policies Categories for Available Timeslots', \"policies_rcp45\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_policies_category(df[df[\"True_Game\"] == 1],'Percentage of Different Policies Categories for Available Timeslots', \"policies_rcp45\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_policies_category(df,'Percentage of Different Policies Categories for Available Timeslots', \"policies_rcp45\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.3 RCP 2.6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/full_UTCI_comparison_2024_2044_corrected.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"Home Team\"] = df[\"Home Team\"].apply(remove_accents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "temp = df[[\"Home Team\",  \"hour\", \"day\", \"month\"]].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df[\"True_Game\"] == 1][[\"Home Team\", \"month\", \"day\", \"hour\", 'True_Game']].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['hour'][df['hour'] == 22] = 21\n",
    "df['hour'][df['hour'] == 20] = 21\n",
    "df['hour'][df['hour'] == 17] = 18\n",
    "df['hour'][df['hour'] == 19] = 18\n",
    "df['hour'][df['hour'] == 16] = 15\n",
    "df['hour'][df['hour'] == 14] = 15\n",
    "\n",
    "df_temp = df[df[\"True_Game\"] == 1][[\"Home Team\", \"month\", \"day\", \"hour\", 'True_Game']].drop_duplicates()\n",
    "nb_by_hour = pd.DataFrame()\n",
    "nb_by_hour[\"hour\"] = df_temp[\"hour\"].drop_duplicates()\n",
    "nb_by_hour['nb_of_occurences']= df_temp['hour'].value_counts().values\n",
    "nb_by_hour\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_copy = df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert 'timestamp' to datetime and extract date and hour\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "# Create a mapping of stress categories to numerical values\n",
    "stress_value_map = {\n",
    "    'No thermal stress': 0,\n",
    "    'Moderate heat stress': 1,\n",
    "    'Strong heat stress': 2,\n",
    "    'Very strong heat stress': 3,\n",
    "    'Extreme heat stress': 4\n",
    "}\n",
    "\n",
    "# Map the stress categories in 'Stress Category 26' to their numerical values\n",
    "df['Stress Value 26'] = df['Stress Category 26'].map(stress_value_map)\n",
    "\n",
    "# Group by location (latitude, longitude), date, and hour, and calculate the sum of average cumulated percentages\n",
    "r_df = df.groupby(['Home Team', 'month','hour'])['Stress Value 26'].mean().reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "r_df.rename(columns={'Stress Value 26': 'Average Stress Value 26'}, inplace=True)\n",
    "\n",
    "r_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime format to extract day and month\n",
    "unique_combinations_df = df_copy\n",
    "unique_combinations_df['date'] = pd.to_datetime(unique_combinations_df['timestamp'])\n",
    "unique_combinations_df['day'] = unique_combinations_df['date'].dt.day\n",
    "unique_combinations_df['month'] = unique_combinations_df['date'].dt.month\n",
    "\n",
    "# Drop the 'date' column as we have separated it into 'day' and 'month'\n",
    "unique_combinations_df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "unique_combinations_df.drop_duplicates(inplace=True)\n",
    "\n",
    "x_df = unique_combinations_df[[\"Home Team\", \"day\", \"month\", \"hour\"]]\n",
    "x_df.drop_duplicates(inplace=True)\n",
    "x_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "o_df = x_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_short = df[[\"Home Team\", \"Capacity\"]].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_short"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dictionary with keys as home team and values as the capacity\n",
    "capacities_dict = df_short.set_index('Home Team')['Capacity'].to_dict()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = gp.Model(\"linear_problem\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "o_df['key'] = list(zip(o_df[\"Home Team\"], o_df[\"month\"], o_df[\"day\"], o_df[\"hour\"]))\n",
    "x_df[\"key\"] = list(zip(x_df[\"Home Team\"], x_df[\"month\"], x_df[\"day\"], x_df[\"hour\"]))\n",
    "\n",
    "# Extract the unique keys\n",
    "unique_keys_o = o_df['key'].unique()\n",
    "unique_keys_x = x_df[\"key\"].unique()\n",
    "\n",
    "# Create binary variables o_d_l using unique keys\n",
    "x_h_d_l = model.addVars(unique_keys_x, vtype=GRB.BINARY, name=\"x\")\n",
    "o_d_l = model.addVars(unique_keys_o, vtype=GRB.BINARY, name=\"o\")\n",
    "\n",
    "value_column_name = r_df.columns[-1]\n",
    "# Create a dictionary for the values\n",
    "values_dict = {(row['hour'], row['month'], row['Home Team']): row[value_column_name]\n",
    "               for index, row in r_df.iterrows()}\n",
    "\n",
    "print(\"vdict\",values_dict)\n",
    "                \n",
    "model.update()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to hold the mapping\n",
    "team_month_day_hours = {}\n",
    "\n",
    "# Iterate through the DataFrame to populate the dictionary\n",
    "for index, row in df.iterrows():\n",
    "    home_team = row['Home Team']\n",
    "    day = row[\"day\"]\n",
    "    month = row[\"month\"]\n",
    "    hour = row['hour']\n",
    "\n",
    "    # Initialize the nested dictionaries if the 'Home Team' or month or day is not already in the dictionary\n",
    "    if home_team not in team_month_day_hours:\n",
    "        team_month_day_hours[home_team] = {}\n",
    "    if month not in team_month_day_hours[home_team]:\n",
    "        team_month_day_hours[home_team][month] = {}\n",
    "    if day not in team_month_day_hours[home_team][month]:\n",
    "        team_month_day_hours[home_team][month][day] = []\n",
    "\n",
    "    # Append the hour to the list of hours available for that 'Home Team', month, and day\n",
    "    if hour not in team_month_day_hours[home_team][month][day]:\n",
    "        team_month_day_hours[home_team][month][day].append(hour)\n",
    "\n",
    "print('full dict', team_month_day_hours)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now we can use this mapping to add constraints to the model\n",
    "\n",
    "# Now we can use this mapping to add constraints to the model\n",
    "unique_keys_x_list = unique_keys_x.tolist()\n",
    "for home_team, months in team_month_day_hours.items():\n",
    "    for month, days in months.items():\n",
    "        for day, hours_list in days.items():\n",
    "            # Ensure that home_team, month, and day are in the correct type before comparison\n",
    "            home_team_str = str(home_team)  # or int(home_team) based on your data\n",
    "            month_int = int(month)\n",
    "            day_int = int(day)\n",
    "\n",
    "            vars_in_constraint = [x_h_d_l[home_team_str, month_int, day_int, h] for h in hours_list if (home_team_str, month_int, day_int, h) in unique_keys_x_list]\n",
    "\n",
    "            model.addConstr(sum(vars_in_constraint) == 1, \"daily_hour_sum_constraint_{}_{}_{}\".format(home_team, month, day))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for hour in nb_by_hour[\"hour\"]:\n",
    "    print(hour, nb_by_hour[nb_by_hour['hour'] == hour]['nb_of_occurences'].values[0])\n",
    "    sum_x_vars = sum(x_h_d_l[l, m, d, hour] for l, m, d, h in x_h_d_l.keys() if h == hour)\n",
    "    model.addConstr(sum_x_vars == nb_by_hour[nb_by_hour['hour'] == hour]['nb_of_occurences'].values[0], f\"hour_constraint_{hour}\")\n",
    "    \n",
    "# Define a large M value\n",
    "M = 1000\n",
    "\n",
    "# Iterate over each unique combination of location, month, day, and hour\n",
    "for l, month_days in team_month_day_hours.items():\n",
    "    for m, days in month_days.items():\n",
    "        for d, hours_list in days.items():\n",
    "            for h in hours_list:\n",
    "                sum_x_vars = sum(x_h_d_l[l2, m, d, h] for l2 in team_month_day_hours if (l2, m, d, h) in unique_keys_x_list)\n",
    "            \n",
    "                # Constraint 1: If two or more games are scheduled, overlapping must be 1\n",
    "                model.addConstr(sum_x_vars <= 1 + M * o_d_l[l, m, d, h],\n",
    "                                f\"overlap_max_constraint_{l}_{m}_{d}_{h}\")\n",
    "                \n",
    "                # Constraint 2: If less than two games are scheduled, overlapping must be 0\n",
    "                model.addConstr(sum_x_vars >= 2 - M * (1 - o_d_l[l, m, d, h]),\n",
    "                                f\"overlap_min_constraint_{l}_{m}_{d}_{h}\")\n",
    "                \n",
    "model.update()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the objective function as a linear expression because r_h_m_l are constants\n",
    "objective = gp.LinExpr()\n",
    "\n",
    "i = 0\n",
    "# Add the sum of products of x_h_d_l and r_h_m_l for matching hour, location, and month\n",
    "\n",
    "for x_key in x_h_d_l.keys():\n",
    "    l, m, d, h = x_key  # x_key is a tuple (h, d, l)\n",
    "    r_key = (h, m, l)\n",
    "    if r_key in values_dict:  # values_dict contains fixed values for r_h_m_l\n",
    "        i += 1\n",
    "        objective.add(x_h_d_l[x_key] * values_dict[r_key] * 0.99995)\n",
    "     \n",
    "# Add the sum of products of o_d_l and c_l for matching location\n",
    "for o_key in o_d_l.keys():\n",
    "    \n",
    "    l,d,m,h = o_key  # o_key is a tuple (d, l)\n",
    "    if l in capacities_dict:  # capacities_dict contains the capacities corresponding to c_l\n",
    "        i+=1\n",
    "        objective.add(o_d_l[o_key] * capacities_dict[l] * 0.00005)\n",
    "\n",
    "\n",
    "# Set the objective function in the model\n",
    "model.update()\n",
    "model.setObjective(objective, GRB.MINIMIZE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_terms = objective.size()\n",
    "print(\"Number of terms in the objective function:\", num_terms)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print all constraints safely, handling potential encoding errors\n",
    "print(\"\\nConstraints:\")\n",
    "constraints = model.getConstrs()\n",
    "for constr in constraints:\n",
    "    lhs_expression = model.getRow(constr)\n",
    "    rhs_value = constr.RHS\n",
    "    print(f\"{constr.ConstrName}: {lhs_expression} = {rhs_value}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Optimize the model\n",
    "model.optimize()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if model.status == GRB.OPTIMAL:\n",
    "    # Retrieve the values of the variables\n",
    "    for v in model.getVars():\n",
    "        print(f'{v.varName}: {v.x}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List to hold the data\n",
    "data = []\n",
    "\n",
    "# Check if the solution exists\n",
    "if model.status == GRB.OPTIMAL:\n",
    "    # Iterate over the variables\n",
    "    for v in model.getVars():\n",
    "        # Check if the variable value is 1\n",
    "        if round(v.x) == 1:\n",
    "            # Parse the variable name\n",
    "            print(v.varName.split(','))\n",
    "            team, month, day, hour = v.varName.split(',')\n",
    "            # Add to the list\n",
    "            if team[0] == 'x':\n",
    "                team = team[2:]\n",
    "                hour = hour[:-1]\n",
    "                data.append([team, int(month), int(day), int(hour)])\n",
    "        else:\n",
    "            print(\"no\")\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df_res = pd.DataFrame(data, columns=['Team', 'Month', 'Day', 'Hour'])\n",
    "else:\n",
    "    print(\"No optimal solution found.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_res[\"optimal_schedule\"] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df = pd.merge(df, df_res, left_on=[\"Home Team\", \"hour\", \"month\", \"day\"], right_on=[\"Team\", \"Hour\", \"Month\", \"Day\"], how=\"inner\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_stress_category(merge_df,'Percentage of Different Stress Categories for optimal game features', 'Stress Category 26')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_stress_category(df[df[\"True_Game\"] == 1],'Percentage of Different Stress Categories for non optimal game features', 'Stress Category 26')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_policies_category(merge_df,'Percentage of Different Policies Categories for Available Timeslots', \"policies_rcp26\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_policies_category(df[df[\"True_Game\"] == 1],'Percentage of Different Policies Categories for Available Timeslots', \"policies_rcp26\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_overall_percentage_policies_category(df,'Percentage of Different Policies Categories for Available Timeslots', \"policies_rcp26\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 8. Comparison Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 8.1 Comparison distribution of UTCI, between the 2 periods 2002-2022 / 2024-2043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_df = pd.read_csv(\"datasets/full_UTCI_comparison_2024_2044_corrected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "past_df = pd.read_csv(\"datasets/UTCI_cat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finding the global minimum and maximum values across all columns\n",
    "min_value = min(past_df['utci'].min(), future_df['UTCI_26'].min(), future_df['UTCI_45'].min(), future_df['UTCI_85'].min())\n",
    "max_value = max(past_df['utci'].max(), future_df['UTCI_26'].max(), future_df['UTCI_45'].max(), future_df['UTCI_85'].max())\n",
    "\n",
    "# Defining a common set of bins\n",
    "bins = np.linspace(min_value, max_value, 20)  # 20 bins\n",
    "\n",
    "# Setting up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting normalized histograms with the same bins\n",
    "plt.hist(past_df['utci'], bins=bins, alpha=1, label='Past UTCI', color='blue', density=True)\n",
    "plt.hist(future_df['UTCI_26'], bins=bins, alpha=0.5, label='Future UTCI_26', color='green', density=True)\n",
    "plt.hist(future_df['UTCI_45'], bins=bins, alpha=0.5, label='Future UTCI_45', color='yellow', density=True)\n",
    "plt.hist(future_df['UTCI_85'], bins=bins, alpha=0.5, label='Future UTCI_85', color='red', density=True)\n",
    "\n",
    "# Removing the grid and setting a white background\n",
    "plt.grid(False)\n",
    "plt.gca().set_facecolor('white')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Normalized Histogram of UTCI values with Common Bins')\n",
    "plt.xlabel('UTCI Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('plots_story/comparison_utci_values_histogram_percentage.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "past_df_copy = past_df\n",
    "future_df_copy = future_df\n",
    "past_df  = past_df[past_df[\"True_Game\"] == 1]\n",
    "future_df  = future_df[future_df[\"True_Game\"] == 1]\n",
    "\n",
    "# Finding the global minimum and maximum values across all columns\n",
    "min_value = min(past_df['utci'].min(), future_df['UTCI_26'].min(), future_df['UTCI_45'].min(), future_df['UTCI_85'].min())\n",
    "max_value = max(past_df['utci'].max(), future_df['UTCI_26'].max(), future_df['UTCI_45'].max(), future_df['UTCI_85'].max())\n",
    "\n",
    "# Defining a common set of bins\n",
    "bins = np.linspace(min_value, max_value, 20)  # 20 bins\n",
    "\n",
    "# Setting up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting normalized histograms with the same bins\n",
    "plt.hist(past_df['utci'], bins=bins, alpha=1, label='Past UTCI', color='blue', density=True)\n",
    "plt.hist(future_df['UTCI_26'], bins=bins, alpha=0.5, label='Future UTCI_26', color='green', density=True)\n",
    "plt.hist(future_df['UTCI_45'], bins=bins, alpha=0.5, label='Future UTCI_45', color='yellow', density=True)\n",
    "plt.hist(future_df['UTCI_85'], bins=bins, alpha=0.5, label='Future UTCI_85', color='red', density=True)\n",
    "\n",
    "# Removing the grid and setting a white background\n",
    "plt.grid(False)\n",
    "plt.gca().set_facecolor('white')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Normalized Histogram of UTCI values with Common Bins for True Games')\n",
    "plt.xlabel('UTCI Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('plots_story/comparison_utci_values_histogram_percentage_true_games.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(future_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 8.2 Comparison percentages categories between the 2 periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculating percentage for each category in 'Stress Category' of past_df\n",
    "past_category_counts = past_df['Stress Category'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Calculating percentages for each category in the relevant columns of future_df\n",
    "future_category_26_counts = future_df['Stress Category 26'].value_counts(normalize=True) * 100\n",
    "future_category_45_counts = future_df['Stress Category 45'].value_counts(normalize=True) * 100\n",
    "future_category_85_counts = future_df['Stress Category 85'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Combining the results into a single dataframe\n",
    "combined_df = pd.DataFrame({\n",
    "    'Past': past_category_counts,\n",
    "    'Future 26': future_category_26_counts,\n",
    "    'Future 45': future_category_45_counts,\n",
    "    'Future 85': future_category_85_counts\n",
    "})\n",
    "\n",
    "# Filling NaN values with 0 (in case some categories are not present in all datasets)\n",
    "combined_df = combined_df.fillna(0)\n",
    "\n",
    "for period in [26, 45, 85]:\n",
    "    combined_df[f\"difference_{period}\"] = combined_df[f\"Future {period}\"] - combined_df[\"Past\"]\n",
    "combined_df.to_csv(\"temp/comparison_heat_stress_percentage.csv\")\n",
    "combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 9. Chi-square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_df = pd.read_csv(\"datasets/full_UTCI_comparison_2024_2044_corrected.csv\")\n",
    "past_df = pd.read_csv(\"datasets/UTCI_cat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "past_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 9.1 chi square for difference in percenatges for the 3 scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = future_df\n",
    "# Check if the 'Stress Category' columns exist in the dataset\n",
    "if 'Stress Category 26' in data.columns and 'Stress Category 45' in data.columns and 'Stress Category 85' in data.columns :\n",
    "    # Calculate the frequency (as percentage) of each category for each stress category column\n",
    "    freq_percent_26 = data['Stress Category 26'].value_counts(normalize=True) * 100\n",
    "    freq_percent_45 = data['Stress Category 45'].value_counts(normalize=True) * 100\n",
    "    freq_percent_85 = data['Stress Category 85'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    # Combine these into a single DataFrame for comparison\n",
    "    combined_percentages = pd.DataFrame({'Stress Category 26': freq_percent_26,\n",
    "                                         'Stress Category 45': freq_percent_45,\n",
    "                                         'Stress Category 85': freq_percent_85}).fillna(0)\n",
    "\n",
    "else:\n",
    "    \"Stress Category columns not found in the dataset.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the raw count of each category for each stress category column\n",
    "count_raw_26 = data['Stress Category 26'].value_counts()\n",
    "count_raw_45 = data['Stress Category 45'].value_counts()\n",
    "count_raw_85 = data['Stress Category 85'].value_counts()\n",
    "\n",
    "# Create a contingency table for the raw counts\n",
    "contingency_table_raw = pd.DataFrame([count_raw_26, count_raw_45, count_raw_85]).fillna(0)\n",
    "\n",
    "# Chi-square test of independence on raw counts\n",
    "chi2_raw, p_raw, dof_raw, expected_raw = chi2_contingency(contingency_table_raw)\n",
    "\n",
    "\n",
    "\n",
    "v = 0\n",
    "c = 0\n",
    "for i in expected_raw:\n",
    "    for j in i:\n",
    "        c += 1\n",
    "        if j > 5:\n",
    "            v += 1\n",
    "print(\"percentage expected value > 5\", (v / c) * 100)\n",
    "chi2_raw, p_raw, dof_raw, expected_raw, (expected_raw > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contingency_table_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the standardized residuals\n",
    "residuals = (contingency_table_raw - expected_raw) / np.sqrt(expected_raw)\n",
    "\n",
    "# Standardized residuals\n",
    "standardized_residuals = residuals / np.sqrt(expected_raw)\n",
    "\n",
    "standardized_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 9.2 chi square for differences in percentages between the 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = future_df\n",
    "data2 = past_df\n",
    "# Check if the 'Stress Category' columns exist in the dataset\n",
    "if 'Stress Category 26' in data.columns and 'Stress Category 45' in data.columns and 'Stress Category 85' in data.columns and 'Stress  Category' in data2.columns:\n",
    "    # Calculate the frequency (as percentage) of each category for each stress category column\n",
    "    freq_percent_26 = data['Stress Category 26'].value_counts(normalize=True) * 100\n",
    "    freq_percent_45 = data['Stress Category 45'].value_counts(normalize=True) * 100\n",
    "    freq_percent_85 = data['Stress Category 85'].value_counts(normalize=True) * 100\n",
    "    freq_percent_past= data2['Stress Category'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    # Combine these into a single DataFrame for comparison\n",
    "    combined_percentages = pd.DataFrame({'Stress Category 26': freq_percent_26,\n",
    "                                         'Stress Category 45': freq_percent_45,\n",
    "                                         'Stress Category 85': freq_percent_85,\n",
    "                                        'Stress Category Past': freq_percent_past}).fillna(0)\n",
    "\n",
    "else:\n",
    "    \"Stress Category columns not found in the dataset.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the raw count of each category for each stress category column\n",
    "count_raw_26 = data['Stress Category 26'].value_counts()\n",
    "count_raw_45 = data['Stress Category 45'].value_counts()\n",
    "count_raw_85 = data['Stress Category 85'].value_counts()\n",
    "count_raw_past = data2['Stress Category'].value_counts()\n",
    "\n",
    "# Create a contingency table for the raw counts\n",
    "contingency_table_raw = pd.DataFrame([count_raw_past,count_raw_26, count_raw_45, count_raw_85]).fillna(0)\n",
    "\n",
    "# Chi-square test of independence on raw counts\n",
    "chi2_raw, p_raw, dof_raw, expected_raw = chi2_contingency(contingency_table_raw)\n",
    "\n",
    "\n",
    "\n",
    "v = 0\n",
    "c = 0\n",
    "\n",
    "for i in expected_raw:\n",
    "    for j in i:\n",
    "        c += 1\n",
    "        if j > 5:\n",
    "            v += 1\n",
    "print(\"percentage expected value > 5\", (v / c) * 100)\n",
    "chi2_raw, p_raw, dof_raw, expected_raw, (expected_raw > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the standardized residuals\n",
    "residuals = (contingency_table_raw - expected_raw) / np.sqrt(expected_raw)\n",
    "\n",
    "# Standardized residuals\n",
    "standardized_residuals = residuals / np.sqrt(expected_raw)\n",
    "\n",
    "standardized_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 9.3 comparison between past and rcp2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the raw count of each category for each stress category column\n",
    "\n",
    "# Create a contingency table for the raw counts\n",
    "contingency_table_raw = pd.DataFrame([count_raw_past,count_raw_26]).fillna(0)\n",
    "\n",
    "# Chi-square test of independence on raw counts\n",
    "chi2_raw, p_raw, dof_raw, expected_raw = chi2_contingency(contingency_table_raw)\n",
    "\n",
    "\n",
    "\n",
    "v = 0\n",
    "c = 0\n",
    "for i in expected_raw:\n",
    "    for j in i:\n",
    "        c += 1\n",
    "        if j > 5:\n",
    "            v += 1\n",
    "print(\"percentage expected value > 5\", (v / c) * 100)\n",
    "chi2_raw, p_raw, dof_raw, expected_raw, (expected_raw > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the standardized residuals\n",
    "residuals = (contingency_table_raw - expected_raw) / np.sqrt(expected_raw)\n",
    "\n",
    "# Standardized residuals\n",
    "standardized_residuals = residuals / np.sqrt(expected_raw)\n",
    "\n",
    "standardized_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 9.4 comparison between past and rcp4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the raw count of each category for each stress category column\n",
    "\n",
    "# Create a contingency table for the raw counts\n",
    "contingency_table_raw = pd.DataFrame([count_raw_past,count_raw_45]).fillna(0)\n",
    "\n",
    "# Chi-square test of independence on raw counts\n",
    "chi2_raw, p_raw, dof_raw, expected_raw = chi2_contingency(contingency_table_raw)\n",
    "\n",
    "\n",
    "\n",
    "v = 0\n",
    "c = 0\n",
    "\n",
    "for i in expected_raw:\n",
    "    for j in i:\n",
    "        c += 1\n",
    "        if j > 5:\n",
    "            v += 1\n",
    "print(\"percentage expected value > 5\", (v / c) * 100)\n",
    "chi2_raw, p_raw, dof_raw, expected_raw, (expected_raw > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the standardized residuals\n",
    "residuals = (contingency_table_raw - expected_raw) / np.sqrt(expected_raw)\n",
    "\n",
    "# Standardized residuals\n",
    "standardized_residuals = residuals / np.sqrt(expected_raw)\n",
    "\n",
    "standardized_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 9.5 comparison between past and rcp8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the raw count of each category for each stress category column\n",
    "\n",
    "# Create a contingency table for the raw counts\n",
    "contingency_table_raw = pd.DataFrame([count_raw_past,count_raw_85]).fillna(0)\n",
    "\n",
    "# Chi-square test of independence on raw counts\n",
    "chi2_raw, p_raw, dof_raw, expected_raw = chi2_contingency(contingency_table_raw)\n",
    "\n",
    "\n",
    "\n",
    "v = 0\n",
    "c = 0\n",
    "\n",
    "for i in expected_raw:\n",
    "    for j in i:\n",
    "        c += 1\n",
    "        if j > 5:\n",
    "            v += 1\n",
    "print(\"percentage expected value > 5\", (v / c) * 100)\n",
    "chi2_raw, p_raw, dof_raw, expected_raw, (expected_raw > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the standardized residuals\n",
    "residuals = (contingency_table_raw - expected_raw) / np.sqrt(expected_raw)\n",
    "\n",
    "# Standardized residuals\n",
    "standardized_residuals = residuals / np.sqrt(expected_raw)\n",
    "\n",
    "standardized_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. Bias Correction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obs_filepath='datasets/UTCI.csv' #observed \n",
    "obs_df=pd.read_csv(obs_filepath)\n",
    "\n",
    "hist_filepath='datasets/UTCI_2002_2023_proj_and_histo.csv' # calculated using models \n",
    "hist_df=pd.read_csv(hist_filepath)\n",
    "\n",
    "\n",
    "futures={}\n",
    "for scenario in [26,45,85]:\n",
    "    future_filepath=f'UTCI_2024_2044_{scenario}.csv'\n",
    "    futures[f'rcp{scenario}']=pd.read_csv(future_filepath)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp = obs_df\n",
    "temp[\"dup\"] = obs_df.duplicated([\"Stadium\", \"timestamp\"])\n",
    "temp[temp[\"dup\"] == True].sort_values([\"Stadium\", \"timestamp\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hist_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obs_df.duplicated().any()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_bc_historical_df=pd.DataFrame({})\n",
    "full_bc_future_df=pd.DataFrame({})\n",
    "\n",
    "for stadium in hist_df['Stadium'].unique():\n",
    "\n",
    "    # Process historical\n",
    "    stadium_df=obs_df[obs_df['Stadium']==stadium].sort_values('True_Game', ascending= False).drop_duplicates(['timestamp'])\n",
    "    obs_time=stadium_df['timestamp'].apply(lambda x: pd.to_datetime(x))\n",
    "    obs_utci=stadium_df['utci']-273.15\n",
    "\n",
    "    stadium_df=hist_df[hist_df['Stadium']==stadium].drop_duplicates(['timestamp'])\n",
    "    hist_time=stadium_df['timestamp'].apply(lambda x: pd.to_datetime(x))\n",
    "    hist_utci=stadium_df['UTCI']\n",
    "\n",
    "    observed_da = xr.DataArray(obs_utci, dims='time', coords={'time': obs_time},attrs={'units': 'degC'})\n",
    "    model_da = xr.DataArray(hist_utci, dims='time', coords={'time': hist_time},attrs={'units': 'degC'})\n",
    "\n",
    "    qdm_mapping=sdba.QuantileDeltaMapping.train(observed_da,model_da)\n",
    "    \n",
    "    corrected_model_da = qdm_mapping.adjust(model_da)\n",
    "\n",
    "    bc_historical_df = corrected_model_da.rename('utci_bc').to_dataframe().reset_index()\n",
    "    for column_to_copy in ['Home Team', 'True_Game','Stadium','latitude','longitude','Capacity','Division']:\n",
    "        bc_historical_df[column_to_copy]=stadium_df.reset_index()[column_to_copy]\n",
    "    \n",
    "    full_bc_historical_df=pd.concat([full_bc_historical_df,bc_historical_df],ignore_index=True)\n",
    "\n",
    "    bc_future_df=pd.DataFrame({})\n",
    "    for scenario in [26,45,85]:\n",
    "        future_df=futures[f'rcp{scenario}']\n",
    "        stadium_df=future_df[future_df['Stadium']==stadium].drop_duplicates(['timestamp'])\n",
    "        future_time=stadium_df['timestamp'].apply(lambda x: pd.to_datetime(x))\n",
    "        future_utci=stadium_df['UTCI']\n",
    "\n",
    "        if scenario==26:\n",
    "            for column_to_copy in ['timestamp','Home Team', 'True_Game','Stadium','latitude','longitude','Capacity','Division']:\n",
    "                bc_future_df[column_to_copy]=stadium_df.reset_index()[column_to_copy]\n",
    "        \n",
    "        model_da = xr.DataArray(future_utci, dims='time', coords={'time': future_time},attrs={'units': 'degC'})\n",
    "        corrected_model_da = qdm_mapping.adjust(model_da)\n",
    "        bc_future_df[f'utci_bc_rcp{scenario}'] = corrected_model_da.rename(f'utci_bc_rcp{scenario}').to_dataframe().reset_index()[f'utci_bc_rcp{scenario}']\n",
    "\n",
    "          \n",
    "        \n",
    "    full_bc_future_df=pd.concat([full_bc_future_df,bc_future_df],ignore_index=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bins = np.arange(-20,51,5)\n",
    "fig,ax=plt.subplots()\n",
    "plt.hist((obs_df['utci']-273.15),bins=bins, histtype='step', color='k', edgecolor='k',linewidth=2)\n",
    "plt.hist(hist_df['UTCI'],bins=bins, histtype='step', color='grey', edgecolor='grey')\n",
    "plt.hist(full_bc_historical_df['utci_bc'],bins=bins, histtype='step', color='orange', edgecolor='orange')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colors_scen={\n",
    "    26:'g',\n",
    "    45:'r',\n",
    "    85:'m'\n",
    "}\n",
    "fig,ax=plt.subplots()\n",
    "plt.hist(full_bc_historical_df['utci_bc'],bins=bins, histtype='step', color='orange', edgecolor='orange')\n",
    "for scenario in [26,45,85]:\n",
    "    plt.hist(full_bc_future_df[f'utci_bc_rcp{scenario}'],bins=bins, histtype='step', color=colors_scen[scenario], edgecolor=colors_scen[scenario])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_bc_historical_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_bc_future_df.to_csv(\"datasets/corrected_future_UTCI.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
